% Unified Bibliography for PhD Manuscript
% Consolidated from genos, htgan, and ftdiffusion projects
@book{embrechts1997,
  title = {Modelling Extremal Events},
  author = {Embrechts, Paul and Kl{\"u}ppelberg, Claudia and Mikosch, Thomas},
  year = 1997,
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-33483-2}
}

@book{haan2006,
  title = {Extreme Value Theory: An Introduction},
  author = {de Haan, Laurens and Ferreira, Ana},
  year = 2006,
  series = {Springer Series in Operations Research},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-23946-0}
}

@book{resnick1987,
  title = {Extreme Values, Regular Variation and Point Processes},
  author = {Resnick, Sidney I.},
  year = 1987,
  series = {Springer Series in Operations Research and Financial Engineering},
  publisher = {Springer},
  address = {New York},
  doi = {10.1007/978-0-387-75953-1}
}

@book{falk2019,
  title = {Multivariate Extreme Value Theory and D-Norms},
  author = {Falk, Michael},
  year = 2019,
  series = {Springer Series in Operations Research and Financial Engineering},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-03819-9}
}

@misc{gudendorf2009,
  title = {Extreme-Value Copulas},
  author = {Gudendorf, Gordon and Segers, Johan},
  year = 2009,
  eprint = {0911.1015},
  archiveprefix = {arXiv},
  note = {To appear in Lecture Notes in Statistics, Springer 2010}
}

@article{charpentier2014,
  title = {Multivariate {Archimax} copulas},
  author = {Charpentier, A. and Foug{\`e}res, A.-L. and Genest, C. and Ne{\v s}lehov{\'a}, J.G.},
  year = 2014,
  journal = {Journal of Multivariate Analysis},
  volume = 126,
  pages = {118--136},
  doi = {10.1016/j.jmva.2013.12.013}
}

% =============================================================================
% ORDER STATISTICS
% =============================================================================

@book{david2003,
  title = {Order Statistics},
  author = {David, Herbert A. and Nagaraja, Haikady N.},
  year = 2003,
  edition = {3rd},
  series = {Wiley Series in Probability and Statistics},
  publisher = {Wiley-Interscience},
  address = {Hoboken, NJ},
  isbn = {978-0-471-38926-2}
}

@inproceedings{grover2019b,
  title = {Stochastic Optimization of Sorting Networks via Continuous Relaxations},
  author = {Grover, Aditya and Wang, Eric and Zweig, Aaron and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations},
  year = 2019
}

@article{sukhatme1937,
  title = {Tests of significance for samples of the $\chi^2$-population with two degrees of freedom},
  author = {Sukhatme, Pandurang V.},
  year = 1937,
  journal = {Annals of Eugenics},
  volume = 8,
  number = 1,
  pages = {52--56}
}

@article{schucany1972,
  title = {Order statistics in simulation},
  author = {Schucany, William R.},
  year = 1972,
  journal = {Journal of Statistical Computation and Simulation},
  volume = 1,
  number = 3,
  pages = {281--286},
  doi = {10.1080/00949657208810024}
}

@article{bhattacharya1974,
  title = {Convergence of Sample Paths of Normalized Sums of Induced Order Statistics},
  author = {Bhattacharya, P. K.},
  year = 1974,
  journal = {The Annals of Statistics},
  volume = 2,
  number = 5,
  doi = {10.1214/aos/1176342823}
}

% =============================================================================
% GENERATIVE ADVERSARIAL NETWORKS
% =============================================================================

@inproceedings{goodfellow2014,
  title = {Generative Adversarial Nets},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
  year = 2014,
  volume = 27,
  publisher = {Curran Associates, Inc.}
}

@inproceedings{arjovsky2017,
  title = {Wasserstein Generative Adversarial Networks},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  series = {Proceedings of Machine Learning Research},
  year = 2017,
  volume = 70,
  pages = {214--223},
  publisher = {PMLR}
}

@inproceedings{gulrajani2017,
  title = {Improved Training of {Wasserstein} {GANs}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  booktitle = {Advances in Neural Information Processing Systems},
  year = 2017,
  volume = 30,
  publisher = {Curran Associates, Inc.}
}

% =============================================================================
% DIFFUSION MODELS AND SCORE MATCHING
% =============================================================================

@misc{ho2020,
  title = {Denoising Diffusion Probabilistic Models},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = 2020,
  eprint = {2006.11239},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}

@inproceedings{song2021,
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle = {International Conference on Learning Representations},
  year = 2021
}

@article{hyvarinen2005,
  title = {Estimation of Non-Normalized Statistical Models by Score Matching},
  author = {Hyv{\"a}rinen, Aapo},
  year = 2005,
  journal = {Journal of Machine Learning Research},
  volume = 6,
  number = 24,
  pages = {695--709}
}

@article{vincent2011,
  title = {A Connection Between Score Matching and Denoising Autoencoders},
  author = {Vincent, Pascal},
  year = 2011,
  journal = {Neural Computation},
  volume = 23,
  number = 7,
  pages = {1661--1674},
  doi = {10.1162/NECO_a_00142}
}

@article{anderson1982,
  title = {Reverse-time diffusion equation models},
  author = {Anderson, Brian D.O.},
  year = 1982,
  journal = {Stochastic Processes and their Applications},
  volume = 12,
  number = 3,
  pages = {313--326},
  doi = {10.1016/0304-4149(82)90051-5}
}

@misc{albergo2023,
  title = {Building Normalizing Flows with Stochastic Interpolants},
  author = {Albergo, Michael S. and Vanden-Eijnden, Eric},
  year = 2023,
  eprint = {2209.15571},
  archiveprefix = {arXiv},
  note = {ICLR 2023}
}

% =============================================================================
% RLHF AND FINE-TUNING
% =============================================================================

@misc{black2024,
  title = {Training Diffusion Models with Reinforcement Learning},
  author = {Black, Kevin and Janner, Michael and Du, Yilun and Kostrikov, Ilya and Levine, Sergey},
  year = 2024,
  eprint = {2305.13301},
  archiveprefix = {arXiv}
}

@misc{clark2024,
  title = {Directly Fine-Tuning Diffusion Models on Differentiable Rewards},
  author = {Clark, Kevin and Vicol, Paul and Swersky, Kevin and Fleet, David J.},
  year = 2024,
  eprint = {2309.17400},
  archiveprefix = {arXiv},
  note = {ICLR 2024}
}

@misc{domingo-enrich2025,
  title = {Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control},
  author = {Domingo-Enrich, Carles and Drozdzal, Michal and Karrer, Brian and Chen, Ricky T. Q.},
  year = 2025,
  eprint = {2409.08861},
  archiveprefix = {arXiv}
}

@misc{fan2023,
  title = {{DPOK}: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models},
  author = {Fan, Ying and Watkins, Olivia and Du, Yuqing and Liu, Hao and Ryu, Moonkyung and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Lee, Kangwook and Lee, Kimin},
  year = 2023,
  eprint = {2305.16381},
  archiveprefix = {arXiv},
  note = {NeurIPS 2023}
}

@misc{bai2022,
  title = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author = {Bai, Yuntao and others},
  year = 2022,
  eprint = {2204.05862},
  archiveprefix = {arXiv}
}

% =============================================================================
% NEURAL NETWORK APPROXIMATION THEORY
% =============================================================================

@article{cybenko1989,
  title = {Approximation by superpositions of a sigmoidal function},
  author = {Cybenko, George},
  year = 1989,
  journal = {Mathematics of Control, Signals, and Systems},
  volume = 2,
  number = 4,
  pages = {303--314},
  doi = {10.1007/BF02551274}
}

@article{hornik1989,
  title = {Multilayer feedforward networks are universal approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = 1989,
  journal = {Neural Networks},
  volume = 2,
  number = 5,
  pages = {359--366},
  doi = {10.1016/0893-6080(89)90020-8}
}

@article{leshno1993,
  title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
  year = 1993,
  journal = {Neural Networks},
  volume = 6,
  number = 6,
  pages = {861--867},
  doi = {10.1016/S0893-6080(05)80131-5}
}

@article{barron1993,
  title = {Universal approximation bounds for superpositions of a sigmoidal function},
  author = {Barron, Andrew R.},
  year = 1993,
  journal = {IEEE Transactions on Information Theory},
  volume = 39,
  number = 3,
  pages = {930--945},
  doi = {10.1109/18.256500}
}

@article{yarotsky2017,
  title = {Error bounds for approximations with deep {ReLU} networks},
  author = {Yarotsky, Dmitry},
  year = 2017,
  journal = {Neural Networks},
  volume = 94,
  pages = {103--114},
  doi = {10.1016/j.neunet.2017.07.002}
}

@inproceedings{montufar2014,
  title = {On the Number of Linear Regions of Deep Neural Networks},
  author = {Montufar, Guido F. and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems},
  year = 2014,
  volume = 27,
  publisher = {Curran Associates, Inc.}
}

@inproceedings{telgarsky2016,
  title = {Benefits of depth in neural networks},
  author = {Telgarsky, Matus},
  booktitle = {Conference on Learning Theory},
  pages = {1517--1539},
  year = 2016,
  publisher = {PMLR}
}

@article{pinkus1999,
  title = {Approximation theory of the {MLP} model in neural networks},
  author = {Pinkus, Allan},
  year = 1999,
  journal = {Acta Numerica},
  volume = 8,
  pages = {143--195},
  doi = {10.1017/S0962492900002919}
}

% =============================================================================
% OPTIMAL TRANSPORT
% =============================================================================

@book{villani2009,
  title = {Optimal Transport: Old and New},
  author = {Villani, C{\'e}dric},
  year = 2009,
  series = {Grundlehren der mathematischen Wissenschaften},
  volume = 338,
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-71050-9}
}

@article{santambrogio2015,
  title = {Optimal transport for applied mathematicians},
  author = {Santambrogio, Filippo},
  year = 2015,
  journal = {Birk{\"a}user, NY},
  volume = 55,
  pages = {94}
}

% =============================================================================
% COPULAS AND DEPENDENCE
% =============================================================================

@book{nelsen2006,
  title = {An Introduction to Copulas},
  author = {Nelsen, Roger B.},
  year = 2006,
  edition = {2nd},
  series = {Springer Series in Statistics},
  publisher = {Springer},
  address = {New York},
  doi = {10.1007/0-387-28678-0}
}

@article{mcneil2009,
  title = {Multivariate {Archimedean} copulas, $d$-monotone functions and $\ell_1$-norm symmetric distributions},
  author = {McNeil, Alexander J. and Ne{\v s}lehov{\'a}, Johanna},
  year = 2009,
  journal = {The Annals of Statistics},
  volume = 37,
  number = {5B},
  pages = {3059--3097},
  doi = {10.1214/07-AOS556}
}

% =============================================================================
% FINANCE AND RISK
% =============================================================================

@book{mcneil2015,
  title = {Quantitative Risk Management: Concepts, Techniques and Tools},
  author = {McNeil, Alexander J. and Frey, R{\"u}diger and Embrechts, Paul},
  year = 2015,
  edition = {Revised},
  series = {Princeton Series in Finance},
  publisher = {Princeton University Press},
  address = {Princeton, NJ}
}

@book{glasserman2003,
  title = {Monte Carlo Methods in Financial Engineering},
  author = {Glasserman, Paul},
  year = 2003,
  series = {Stochastic Modelling and Applied Probability},
  volume = 53,
  publisher = {Springer},
  address = {New York},
  doi = {10.1007/978-0-387-21617-1}
}

@article{cont2001,
  title = {Empirical Properties of Asset Returns: Stylized Facts and Statistical Issues},
  author = {Cont, Rama},
  journal = {Quantitative Finance},
  volume = 1,
  number = 2,
  pages = {223--236},
  year = 2001,
  publisher = {Taylor \& Francis},
  doi = {10.1080/713665670}
}

% =============================================================================
% VAE AND NORMALIZING FLOWS
% =============================================================================

@misc{kingma2014,
  title = {Auto-Encoding Variational {Bayes}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = 2014,
  eprint = {1312.6114},
  archiveprefix = {arXiv}
}

@inproceedings{rezende2015,
  title = {Variational Inference with Normalizing Flows},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  year = 2015,
  pages = {1530--1538},
  publisher = {PMLR}
}

% =============================================================================
% OWN WORKS
% =============================================================================

@misc{girard2024,
  title = {Deep generative modeling of multivariate dependent extremes},
  author = {Girard, St{\'e}phane and Gobet, Emmanuel and Pachebat, Jean},
  year = 2024,
  note = {Working paper, HAL-04700084}
}

% =============================================================================
% ADDITIONAL EXTREME VALUE THEORY REFERENCES
% =============================================================================

@article{gumbel1960,
  title = {Bivariate exponential distributions},
  author = {Gumbel, Emil J.},
  year = 1960,
  journal = {Journal of the American Statistical Association},
  volume = 55,
  number = 292,
  pages = {698--707},
  doi = {10.1080/01621459.1960.10483368}
}

@article{husler1989,
  title = {Maxima of normal random vectors: between independence and complete dependence},
  author = {H{\"u}sler, J{\"u}rg and Reiss, Rolf-Dieter},
  year = 1989,
  journal = {Statistics \& Probability Letters},
  volume = 7,
  number = 4,
  pages = {283--286},
  doi = {10.1016/0167-7152(89)90106-5}
}

@article{nikoloulopoulos2009,
  title = {Extreme value properties of multivariate $t$ copulas},
  author = {Nikoloulopoulos, Aristidis K. and Joe, Harry and Li, Haijun},
  year = 2009,
  journal = {Extremes},
  volume = 12,
  number = 2,
  pages = {129--148},
  doi = {10.1007/s10687-008-0072-4}
}

@article{einmahl2012,
  title = {An {M}-estimator for tail dependence in arbitrary dimensions},
  author = {Einmahl, John H. J. and Krajina, Andrea and Segers, Johan},
  year = 2012,
  journal = {The Annals of Statistics},
  volume = 40,
  number = 3,
  pages = {1764--1793},
  doi = {10.1214/12-AOS1023}
}

@article{einmahl2016,
  title = {Statistics of heteroscedastic extremes},
  author = {Einmahl, John H. J. and de Haan, Laurens and Zhou, Chen},
  year = 2016,
  journal = {Journal of the Royal Statistical Society: Series B},
  volume = 78,
  number = 1,
  pages = {31--51},
  doi = {10.1111/rssb.12099}
}

@article{sabourin2013,
  title = {Bayesian Dirichlet mixture model for multivariate extremes: A re-parametrization},
  author = {Sabourin, Anne and Naveau, Philippe and Fougères, Anne-Laure},
  year = 2013,
  journal = {Computational Statistics \& Data Analysis},
  volume = 71,
  pages = {542--567},
  doi = {10.1016/j.csda.2013.04.021}
}

@misc{wiese2019,
  title = {Deep Hedging: Learning to Simulate Equity Option Markets},
  author = {Wiese, Magnus and Knobloch, Robert and Korn, Ralf and Kretschmer, Peter},
  year = 2019,
  eprint = {1911.01700},
  archiveprefix = {arXiv}
}

@misc{allouche2022,
  title = {Robust Estimation for Extremal Dependence},
  author = {Allouche, Micha{\"e}l and Girard, St{\'e}phane and Gobet, Emmanuel},
  year = 2022,
  eprint = {2206.10876},
  archiveprefix = {arXiv}
}

@book{coles2001,
  title = {An Introduction to Statistical Modeling of Extreme Values},
  author = {Coles, Stuart},
  year = 2001,
  series = {Springer Series in Statistics},
  publisher = {Springer},
  address = {London},
  doi = {10.1007/978-1-4471-3675-0}
}

% =============================================================================
% ADDITIONAL ORDER STATISTICS REFERENCES
% =============================================================================

@book{arnold2008,
  title = {A First Course in Order Statistics},
  author = {Arnold, Barry C. and Balakrishnan, Narayanaswamy and Nagaraja, Haikady N.},
  year = 2008,
  edition = {Classics in Applied Mathematics},
  publisher = {SIAM},
  address = {Philadelphia},
  doi = {10.1137/1.9780898719062}
}

@article{berg2022,
  title = {Aggregate Confusion: The Divergence of {ESG} Ratings},
  author = {Berg, Florian and K{\"o}lbel, Julian F. and Rigobon, Roberto},
  year = 2022,
  journal = {Review of Finance},
  volume = 26,
  number = 6,
  pages = {1315--1344},
  doi = {10.1093/rof/rfac033}
}

@book{krishna2009,
  title = {Auction Theory},
  author = {Krishna, Vijay},
  year = 2009,
  edition = {2nd},
  publisher = {Academic Press},
  address = {San Diego}
}

% =============================================================================
% ADDITIONAL MACHINE LEARNING REFERENCES
% =============================================================================

@book{liu2009,
  title = {Learning to Rank for Information Retrieval},
  author = {Liu, Tie-Yan},
  year = 2009,
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-14267-3}
}

@inproceedings{cuturi2019,
  title = {Differentiable Ranking and Sorting using Optimal Transport},
  author = {Cuturi, Marco and Teboul, Olivier and Vert, Jean-Philippe},
  booktitle = {Advances in Neural Information Processing Systems},
  year = 2019,
  volume = 32,
  publisher = {Curran Associates, Inc.}
}

@inproceedings{blondel2020,
  title = {Fast Differentiable Sorting and Ranking},
  author = {Blondel, Mathieu and Teboul, Olivier and Berthet, Quentin and Djolonga, Josip},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  year = 2020,
  pages = {950--959},
  publisher = {PMLR}
}

% =============================================================================
% ADDITIONAL RLHF REFERENCES
% =============================================================================

@misc{christiano2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = 2017,
  eprint = {1706.03741},
  archiveprefix = {arXiv},
  note = {NeurIPS 2017}
}

@misc{ouyang2022,
  title = {Training language models to follow instructions with human feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  year = 2022,
  eprint = {2203.02155},
  archiveprefix = {arXiv},
  note = {NeurIPS 2022}
}

@misc{casper2023,
  title = {Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback},
  author = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  year = 2023,
  eprint = {2307.15217},
  archiveprefix = {arXiv}
}

@misc{xu2024,
  title = {{ReFL}: Reflective Feedback Learning for Language Model Alignment},
  author = {Xu, Jing and Liu, Andrew and Wu, Andrea and Saeedi, Parisa and Neville, Jennifer and Diab, Mona and Celikyilmaz, Asli},
  year = 2024,
  eprint = {2403.01673},
  archiveprefix = {arXiv}
}

% =============================================================================
% HEAVY-TAILED GENERATIVE MODELS
% =============================================================================

@incollection{allouche_chapterextreme,
  title = {Extreme Event Simulation Using Generative Adversarial Networks},
  author = {Allouche, Michaël and Gobet, Emmanuel and Girard, Stéphane},
  booktitle = {Handbook of Statistics},
  year = 2024,
  publisher = {Elsevier},
  note = {Chapter on extreme value GAN methods}
}

@article{boulaguiem2022,
  title = {Modeling and simulating spatial extremes by combining extreme value theory with generative adversarial networks},
  author = {Boulaguiem, Youssef and Zscheischler, Jakob and Vignotto, Edoardo and van der Wiel, Karin and Engelke, Sebastian},
  journal = {Environmental Data Science},
  volume = 1,
  pages = {e5},
  year = 2022,
  doi = {10.1017/eds.2022.4}
}

@misc{huster2021pareto,
  title = {Pareto {GAN}: Extending the Representational Power of {GAN}s to Heavy-Tailed Distributions},
  author = {Huster, Todd and Cohen, Jeremy and Lin, Zinan and Chan, Kevin and Kamhoua, Charles and Kundu, Nandi and Kwiat, Kevin and Leslie, Noel},
  year = 2021,
  eprint = {2101.09113},
  archiveprefix = {arXiv}
}

@article{lo2021,
  title = {Quantifying the Impact of Impact Investing},
  author = {Lo, Andrew W. and Zhang, Ruixun},
  year = 2021,
  journal = {Management Science},
  doi = {10.2139/ssrn.3944367}
}

% =============================================================================
% DENSITY ESTIMATION AND MCMC
% =============================================================================

@book{silverman1986,
  title = {Density Estimation for Statistics and Data Analysis},
  author = {Silverman, Bernard W.},
  year = 1986,
  publisher = {Chapman and Hall},
  address = {London},
  doi = {10.1007/978-1-4899-3324-9}
}

@book{scott2015,
  title = {Multivariate Density Estimation: Theory, Practice, and Visualization},
  author = {Scott, David W.},
  year = 2015,
  edition = {2nd},
  publisher = {Wiley},
  address = {Hoboken, NJ},
  doi = {10.1002/9781118575574}
}

@article{metropolis1953,
  title = {Equation of State Calculations by Fast Computing Machines},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  year = 1953,
  journal = {The Journal of Chemical Physics},
  volume = 21,
  number = 6,
  pages = {1087--1092},
  doi = {10.1063/1.1699114}
}

@article{hastings1970,
  title = {Monte {Carlo} Sampling Methods Using {Markov} Chains and Their Applications},
  author = {Hastings, W. K.},
  year = 1970,
  journal = {Biometrika},
  volume = 57,
  number = 1,
  pages = {97--109},
  doi = {10.1093/biomet/57.1.97}
}

@book{robert2004,
  title = {Monte Carlo Statistical Methods},
  author = {Robert, Christian P. and Casella, George},
  year = 2004,
  edition = {2nd},
  publisher = {Springer},
  address = {New York},
  doi = {10.1007/978-1-4757-4145-2}
}

@book{bellman1961,
  title = {Adaptive Control Processes: A Guided Tour},
  author = {Bellman, Richard},
  year = 1961,
  publisher = {Princeton University Press},
  address = {Princeton, NJ}
}

@inproceedings{dinh2017,
  title = {Density estimation using {Real-NVP}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  booktitle = {International Conference on Learning Representations},
  year = 2017
}

% =============================================================================
% CLIMATE EXTREMES
% =============================================================================

@article{zscheischler2018,
  title = {Future climate risk from compound events},
  author = {Zscheischler, Jakob and Westra, Seth and van den Hurk, Bart J. J. M. and Seneviratne, Sonia I. and Ward, Philip J. and Pitman, Andy and AghaKouchak, Amir and Bresch, David N. and Leonard, Michael and Wahl, Thomas and Zhang, Xuebin},
  journal = {Nature Climate Change},
  volume = 8,
  pages = {469--477},
  year = 2018,
  doi = {10.1038/s41558-018-0156-3}
}

@article{bevacqua2021,
  title = {Guidelines for studying diverse types of compound weather and climate events},
  author = {Bevacqua, Emanuele and De Michele, Carlo and Manning, Colin and Couasnon, Anais and Ribeiro, Andreia F. S. and Ramos, Alexandre M. and Vignotto, Edoardo and Bastos, Ana and Blesic, Suzana and Durante, Fabrizio and others},
  journal = {Earth's Future},
  volume = 9,
  number = 11,
  pages = {e2021EF002340},
  year = 2021,
  doi = {10.1029/2021EF002340}
}

@misc{yoon2023,
  title = {Score-based Generative Modeling through Stochastic Evolution Equations in Hilbert Spaces},
  author = {Yoon, Sungbin and Park, Juho and Lee, Jaehyeon and Kim, Gunhee},
  year = 2023,
  eprint = {2305.19241},
  archiveprefix = {arXiv},
  note = {Heavy-tailed diffusion models using L\'evy processes}
}
@misc{drees2019principal,
      title={Principal Component Analysis for Multivariate Extremes}, 
      author={Holger Drees and Anne Sabourin},
      year={2019},
      eprint={1906.11043},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/start},
	urldate = {2023-05-16},
	file = {Zotero | Your personal research assistant:/Users/jeanpachebat/Zotero/storage/Z2MA7BDT/start.html:text/html},
}

@misc{gudendorf_nonparametric_2011,
	title = {Nonparametric estimation of multivariate extreme-value copulas},
	url = {http://arxiv.org/abs/1107.2410},
	abstract = {Extreme-value copulas arise in the asymptotic theory for componentwise maxima of independent random samples. An extreme-value copula is determined by its Pickands dependence function, which is a function on the unit simplex subject to certain shape constraints that arise from an integral transform of an underlying measure called spectral measure. Multivariate extensions are provided of certain rank-based nonparametric estimators of the Pickands dependence function. The shape constraint that the estimator should itself be a Pickands dependence function is enforced by replacing an initial estimator by its best least-squares approximation in the set of Pickands dependence functions having a discrete spectral measure supported on a sufficiently fine grid. Weak convergence of the standardized estimators is demonstrated and the finite-sample performance of the estimators is investigated by means of a simulation experiment.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Gudendorf, Gordon and Segers, Johan},
	month = nov,
	year = {2011},
	note = {arXiv:1107.2410},
	keywords = {62G05, 62G32, 62G20, Statistics - Methodology},
	annote = {Comment: 26 pages; submitted; Universit{\textbackslash}'e catholique de Louvain, Institut de statistique, biostatistique et sciences actuarielles},
	file = {arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/4RT78NU8/1107.html:text/html;Full Text PDF:/Users/jeanpachebat/Zotero/storage/TDHDKMLS/Gudendorf and Segers - 2011 - Nonparametric estimation of multivariate extreme-v.pdf:application/pdf},
}

@misc{noauthor_dense_nodate,
	title = {Dense classes of multivariate extreme value distributions {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0047259X12002746?token=C9B035E67CB54F611BDCCC4BC1EA1998E7A40A30C20CDB02F30D229D1D343C59D6845740035ED48ABC7066ED3B5DEF9E&originRegion=eu-west-1&originCreation=20230516093039},
	language = {en},
	urldate = {2023-05-16},
	doi = {10.1016/j.jmva.2012.11.015},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/JGS9VT6G/Dense classes of multivariate extreme value distri.pdf:application/pdf},
}

@misc{gudendorf_extreme-value_2009,
	title = {Extreme-{Value} {Copulas}},
	url = {http://arxiv.org/abs/0911.1015},
	abstract = {Being the limits of copulas of componentwise maxima in independent random samples, extreme-value copulas can be considered to provide appropriate models for the dependence structure between rare events. Extreme-value copulas not only arise naturally in the domain of extreme-value theory, they can also be a convenient choice to model general positive dependence structures. The aim of this survey is to present the reader with the state-of-the-art in dependence modeling via extreme-value copulas. Both probabilistic and statistical issues are reviewed, in a nonparametric as well as a parametric context.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Gudendorf, Gordon and Segers, Johan},
	month = dec,
	year = {2009},
	note = {arXiv:0911.1015 [math, stat]},
	keywords = {62G32, 62H20, Mathematics - Statistics Theory},
	annote = {Comment: 20 pages, 3 figures. Minor revision, typos corrected. To appear in F. Durante, W. Haerdle, P. Jaworski, and T. Rychlik (editors) "Workshop on Copula Theory and its Applications", Lecture Notes in Statistics -- Proceedings, Springer 2010},
	file = {arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/C9GFR2VF/0911.html:text/html;Full Text PDF:/Users/jeanpachebat/Zotero/storage/I44S5B5V/Gudendorf and Segers - 2009 - Extreme-Value Copulas.pdf:application/pdf},
}

@misc{segers_max-stable_2012,
	title = {Max-stable models for multivariate extremes},
	url = {http://arxiv.org/abs/1204.0332},
	abstract = {Multivariate extreme-value analysis is concerned with the extremes in a multivariate random sample, that is, points of which at least some components have exceptionally large values. Mathematical theory suggests the use of max-stable models for univariate and multivariate extremes. A comprehensive account is given of the various ways in which max-stable models are described. Furthermore, a construction device is proposed for generating parametric families of max-stable distributions. Although the device is not new, its role as a model generator seems not yet to have been fully exploited.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Segers, Johan},
	month = apr,
	year = {2012},
	note = {arXiv:1204.0332 [math, stat]},
	keywords = {Statistics - Methodology, 60G70, 62G32, Mathematics - Probability},
	annote = {Comment: Invited paper for RevStat Statistical Journal. 22 pages, 3 figures},
	file = {arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/R9RX4NKW/1204.html:text/html;Full Text PDF:/Users/jeanpachebat/Zotero/storage/W4E828JP/Segers - 2012 - Max-stable models for multivariate extremes.pdf:application/pdf},
}

@article{krupskii_factor_2013,
	title = {Factor copula models for multivariate data},
	volume = {120},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X13000870},
	doi = {10.1016/j.jmva.2013.05.001},
	abstract = {General conditional independence models for d observed variables, in terms of p latent variables, are presented in terms of bivariate copulas that link observed data to latent variables. The representation is called a factor copula model and the classical multivariate normal model with a correlation matrix having a factor structure is a special case. Dependence and tail properties of the model are obtained. The factor copula model can handle multivariate data with tail dependence and tail asymmetry, properties that the multivariate normal copula does not possess. It is a good choice for modeling high-dimensional data as a parametric form can be specified to have O(d) dependence parameters instead of O(d2) parameters. Data examples show that, based on the Akaike information criterion, the factor copula model provides a good fit to financial return data, in comparison with related truncated vine copula models.},
	language = {en},
	urldate = {2023-05-16},
	journal = {Journal of Multivariate Analysis},
	author = {Krupskii, Pavel and Joe, Harry},
	month = sep,
	year = {2013},
	pages = {85--101},
	file = {Krupskii and Joe - 2013 - Factor copula models for multivariate data.pdf:/Users/jeanpachebat/Zotero/storage/JRSIRW6L/Krupskii and Joe - 2013 - Factor copula models for multivariate data.pdf:application/pdf},
}

@misc{uscidda_monge_2023,
	title = {The {Monge} {Gap}: {A} {Regularizer} to {Learn} {All} {Transport} {Maps}},
	shorttitle = {The {Monge} {Gap}},
	url = {http://arxiv.org/abs/2302.04953},
	abstract = {Optimal transport (OT) theory has been been used in machine learning to study and characterize maps that can push-forward efﬁciently a probability measure onto another. Recent works have drawn inspiration from Brenier’s theorem, which states that when the ground cost is the squaredEuclidean distance, the “best” map to morph a continuous measure in P(Rd) into another must be the gradient of a convex function. To exploit that result, Makkuva et al. (2020); Korotin et al. (2020) consider maps T = ∇fθ, where fθ is an input convex neural network (ICNN), as deﬁned by Amos et al. (2017), and ﬁt θ with SGD using samples. Despite their mathematical elegance, ﬁtting OT maps with ICNNs raises many challenges, due notably to the many constraints imposed on θ; the need to approximate the conjugate of fθ; or the limitation that they only work for the squaredEuclidean cost. More generally, we question the relevance of using Brenier’s result, which only applies to densities, to constrain the architecture of candidate maps ﬁtted on samples. Motivated by these limitations, we propose a radically different approach to estimating OT maps: Given a cost c and a reference measure ρ, we introduce a regularizer, the Monge gap Mcρ(T ) of a map T . That gap quantiﬁes how far a map T deviates from the ideal properties we expect from a c-OT map. In practice, we drop all architecture requirements for T and simply minimize a distance (e.g., the Sinkhorn divergence) between T µ and ν, regularized by Mcρ(T ). We study Mcρ, and show how our simple pipeline outperforms signiﬁcantly other baselines in practice.},
	language = {en},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Uscidda, Théo and Cuturi, Marco},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04953 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uscidda and Cuturi - 2023 - The Monge Gap A Regularizer to Learn All Transpor.pdf:/Users/jeanpachebat/Zotero/storage/NCF2D392/Uscidda and Cuturi - 2023 - The Monge Gap A Regularizer to Learn All Transpor.pdf:application/pdf},
}

@article{charpentier_multivariate_2014,
	title = {Multivariate {Archimax} copulas},
	volume = {126},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X14000074},
	doi = {10.1016/j.jmva.2013.12.013},
	abstract = {A multivariate extension of the bivariate class of Archimax copulas was recently proposed by Mesiar and Jágr (2013), who asked under which conditions it holds. This paper answers their question and provides a stochastic representation of multivariate Archimax copulas. A few basic properties of these copulas are explored, including their minimum and maximum domains of attraction. Several non-trivial examples of multivariate Archimax copulas are also provided.},
	language = {en},
	urldate = {2023-05-16},
	journal = {Journal of Multivariate Analysis},
	author = {Charpentier, A. and Fougères, A.-L. and Genest, C. and Nešlehová, J.G.},
	month = apr,
	year = {2014},
	pages = {118--136},
	file = {Charpentier et al. - 2014 - Multivariate Archimax copulas.pdf:/Users/jeanpachebat/Zotero/storage/93RL9ZWR/Charpentier et al. - 2014 - Multivariate Archimax copulas.pdf:application/pdf},
}

@article{mcneil_multivariate_2009,
	title = {Multivariate {Archimedean} copulas, d-monotone functions and ℓ1-norm symmetric distributions},
	volume = {37},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-37/issue-5B/Multivariate-Archimedean-copulas-d-monotone-functions-and-%e2%84%931-norm-symmetric/10.1214/07-AOS556.full},
	doi = {10.1214/07-AOS556},
	language = {en},
	number = {5B},
	urldate = {2023-05-16},
	journal = {The Annals of Statistics},
	author = {A.J. McNeil and J. Nešlehová},
	month = oct,
	year = {2009},
	file = {McNeil and Nešlehová - 2009 - Multivariate Archimedean copulas, d-monotone funct.pdf:/Users/jeanpachebat/Zotero/storage/L3PVF5VQ/McNeil and Nešlehová - 2009 - Multivariate Archimedean copulas, d-monotone funct.pdf:application/pdf},
}

@article{charpentier_tails_2009,
	title = {Tails of multivariate {Archimedean} copulas},
	volume = {100},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X08002790},
	doi = {10.1016/j.jmva.2008.12.015},
	abstract = {A complete and user-friendly directory of tails of Archimedean copulas is presented which can be used in the selection and construction of appropriate models with desired properties. The results are synthesized in the form of a decision tree: Given the values of some readily computable characteristics of the Archimedean generator, the upper and lower tails of the copula are classified into one of three classes each, one corresponding to asymptotic dependence and the other two to asymptotic independence. For a long list of single-parameter families, the relevant tail quantities are computed so that the corresponding classes in the decision tree can easily be determined. In addition, new models with tailor-made upper and lower tails can be constructed via a number of transformation methods. The frequently occurring category of asymptotic independence turns out to conceal a surprisingly rich variety of tail dependence structures.},
	language = {en},
	number = {7},
	urldate = {2023-05-16},
	journal = {Journal of Multivariate Analysis},
	author = {A. Charpentier and J. Segers},
	month = aug,
	year = {2009},
	pages = {1521--1537},
	file = {Charpentier and Segers - 2009 - Tails of multivariate Archimedean copulas.pdf:/Users/jeanpachebat/Zotero/storage/XUW7BTKK/Charpentier and Segers - 2009 - Tails of multivariate Archimedean copulas.pdf:application/pdf},
}

@article{albrecher_tail_2006,
	title = {Tail asymptotics for the sum of two heavy-tailed dependent risks},
	volume = {9},
	issn = {1386-1999, 1572-915X},
	url = {http://link.springer.com/10.1007/s10687-006-0011-1},
	doi = {10.1007/s10687-006-0011-1},
	abstract = {Let X1, X2 denote positive heavy-tailed random variables with continuous marginal distribution functions F1 and F2, respectively. The asymptotic behavior of the tail of X1+X2 is studied in a general copula framework and some bounds and extremal properties are provided. For more specific assumptions on F1, F2 and the underlying dependence structure of X1 and X2, we survey explicit asymptotic results available in the literature and add several new cases.},
	language = {en},
	number = {2},
	urldate = {2023-05-16},
	journal = {Extremes},
	author = {Albrecher, Hansjörg and Asmussen, Søren and Kortschak, Dominik},
	month = nov,
	year = {2006},
	pages = {107--130},
	file = {Albrecher et al. - 2006 - Tail asymptotics for the sum of two heavy-tailed d.pdf:/Users/jeanpachebat/Zotero/storage/WYREBFFD/Albrecher et al. - 2006 - Tail asymptotics for the sum of two heavy-tailed d.pdf:application/pdf},
}

@article{barbe_tail_nodate,
	title = {{ON} {THE} {TAIL} {BEHAVIOR} {OF} {SUMS} {OF} {DEPENDENT} {RISKS}},
	abstract = {The tail behavior of sums of dependent risks was considered by Wüthrich (2003) and by Alink et al. (2004, 2005) in the case where the variables are exchangeable and connected through an Archimedean copula model. It is shown here how their result can be extended to a broader class of dependence structures using multivariate extreme-value theory. An explicit form is given for the asymptotic probability of extremal events, and the behavior of the latter is studied as a function of the indices of regular variation of both the copula and the common distribution of the risks.},
	language = {en},
	author = {Barbe, Philippe and Fougères, Anne-Laure and Genest, Christian},
	file = {Barbe et al. - ON THE TAIL BEHAVIOR OF SUMS OF DEPENDENT RISKS.pdf:/Users/jeanpachebat/Zotero/storage/WUH9UAYX/Barbe et al. - ON THE TAIL BEHAVIOR OF SUMS OF DEPENDENT RISKS.pdf:application/pdf},
}


@incollection{kortschak_asymptotic_2009,
author={C. M. Goldie and C. Kl\"uppelberg},
year={1998},
title={Subexponential distributions},
editor={R.J. Adler and R.J. Feldman and M.S. Taqqu},
pages={435--459},
address={Boston},
  publisher={Birkhauser},
booktitle={A Practical guide to heavy tails}
}

@article{kloppelberg_large_2023,
	title = {Large {Deviations} of {Heavy}-{Tailed} {Random} {Sums} with {Applications} in {Insurance} and {Finance}},
	abstract = {We prove large deviation results for the random sum S(t)=N{\textasciitilde} X,\_ t {\textgreater} 0, whe (N(t)),?o are non-negative integer-valued random variables and (X,),,N are i.i. non-negative random variables with common distribution function F, independent (N(t)),{\textgreater}o. Special attention is paid to the compound Poisson process and its ramificatio The right tail of the distribution function F is supposed to be of Pareto type (regula or extended regularly varying). The large deviation results are applied to certain proble in insurance and finance which are related to large claims.},
	language = {en},
	author = {Kloppelberg, C and Mikosch, T},
	year = {2023},
	file = {Kloppelberg and Mikosch - 2023 - Large Deviations of Heavy-Tailed Random Sums with .pdf:/Users/jeanpachebat/Zotero/storage/RTV6HN29/Kloppelberg and Mikosch - 2023 - Large Deviations of Heavy-Tailed Random Sums with .pdf:application/pdf},
}

@misc{barbe_asymptotic_2004,
	title = {Asymptotic expansions for infinite weighted convolutions of heavy tail distributions and applications},
	url = {http://arxiv.org/abs/math/0412537},
	abstract = {We establish some asymptotic expansions for infinite weighted convolution of distributions having regular varying tails. Various applications to statistics and probability are developed.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Barbe, Ph and McCormick, W. P.},
	month = dec,
	year = {2004},
	note = {arXiv:math/0412537},
	keywords = {Mathematics - Statistics Theory, Mathematics - Probability, 41A60, 60F99 (primary) 41A80, 44A35, 60E07, 60G50, 60K05, 60K25, 62E17, 62G32 (secondary)},
	annote = {Comment: 125 pages},
	file = {arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/M2BD2QWJ/0412537.html:text/html;Full Text PDF:/Users/jeanpachebat/Zotero/storage/MKIVJNJF/Barbe and McCormick - 2004 - Asymptotic expansions for infinite weighted convol.pdf:application/pdf},
}

@book{leadbetter_extremes_1983,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Extremes and {Related} {Properties} of {Random} {Sequences} and {Processes}},
	isbn = {978-1-4612-5451-5 978-1-4612-5449-2},
	url = {http://link.springer.com/10.1007/978-1-4612-5449-2},
	language = {en},
	urldate = {2023-05-16},
	publisher = {Springer New York},
	author = {Leadbetter, M. R. and Lindgren, Georg and Rootzén, Holger},
	year = {1983},
	doi = {10.1007/978-1-4612-5449-2},
	file = {Leadbetter et al. - 1983 - Extremes and Related Properties of Random Sequence.pdf:/Users/jeanpachebat/Zotero/storage/QSWLL5NG/Leadbetter et al. - 1983 - Extremes and Related Properties of Random Sequence.pdf:application/pdf},
}

@book{embrechts_modelling_1997,
	address = {Berlin, Heidelberg},
	title = {Modelling {Extremal} {Events}},
	isbn = {978-3-642-08242-9 978-3-642-33483-2},
	url = {http://link.springer.com/10.1007/978-3-642-33483-2},
	language = {en},
	urldate = {2023-05-16},
	publisher = {Springer},
	author = {Embrechts, P. and Klüppelberg, C. and Mikosch, T.},
	year = {1997},
	doi = {10.1007/978-3-642-33483-2},
	file = {Embrechts et al. - 1997 - Modelling Extremal Events.pdf:/Users/jeanpachebat/Zotero/storage/V32TTFEX/Embrechts et al. - 1997 - Modelling Extremal Events.pdf:application/pdf},
}

@book{haan_extreme_2006,
	address = {New York ; London},
	series = {Springer Series in Operations Research},
	title = {Extreme value theory: an introduction},
	isbn = {978-0-387-23946-0},
	shorttitle = {Extreme value theory},
	language = {en},
	publisher = {Springer},
	author = {Haan, L. de and Ferreira, Ana},
	year = {2006},
	keywords = {Extreme value theory},
	file = {Haan and Ferreira - 2006 - Extreme value theory an introduction.pdf:/Users/jeanpachebat/Zotero/storage/Z9Z2TDNT/Haan and Ferreira - 2006 - Extreme value theory an introduction.pdf:application/pdf},
}

@book{resnick_extreme_1987,
	address = {New York, NY},
	series = {Springer {Series} in {Operations} {Research} and {Financial} {Engineering}},
	title = {Extreme {Values}, {Regular} {Variation} and {Point} {Processes}},
	isbn = {978-0-387-75952-4},
	url = {http://link.springer.com/10.1007/978-0-387-75953-1},
	language = {en},
	urldate = {2023-05-16},
	publisher = {Springer},
	author = {Resnick, S. I.},
	year = {1987},
	doi = {10.1007/978-0-387-75953-1},
	file = {Resnick - 1987 - Extreme Values, Regular Variation and Point Proces.pdf:/Users/jeanpachebat/Zotero/storage/GXKU6HQ7/Resnick - 1987 - Extreme Values, Regular Variation and Point Proces.pdf:application/pdf},
}

@book{falk_laws_2011,
	address = {Basel},
	title = {Laws of {Small} {Numbers}: {Extremes} and {Rare} {Events}},
	isbn = {978-3-0348-0008-2 978-3-0348-0009-9},
	shorttitle = {Laws of {Small} {Numbers}},
	url = {https://link.springer.com/10.1007/978-3-0348-0009-9},
	language = {en},
	urldate = {2023-05-16},
	publisher = {Springer Basel},
	author = {M. Falk and J. Hüsler and R. Reiss},
	year = {2011},
	doi = {10.1007/978-3-0348-0009-9},
	file = {Falk et al. - 2011 - Laws of Small Numbers Extremes and Rare Events.pdf:/Users/jeanpachebat/Zotero/storage/9L6TTIEN/Falk et al. - 2011 - Laws of Small Numbers Extremes and Rare Events.pdf:application/pdf},
}

@article{benezet_transform_2023,
	title = {Transform {MCMC} {Schemes} for {Sampling} {Intractable} {Factor} {Copula} {Models}},
	volume = {25},
	issn = {1573-7713},
	url = {https://doi.org/10.1007/s11009-023-09983-4},
	doi = {10.1007/s11009-023-09983-4},
	abstract = {In financial risk management, modelling dependency within a random vector \$\${\textbackslash}mathcal\{X\}\$\$is crucial, a standard approach is the use of a copula model. Say the copula model can be sampled through realizations of \$\${\textbackslash}mathcal\{Y\}\$\$having copula function C: had the marginals of \$\${\textbackslash}mathcal\{Y\}\$\$been known, sampling \$\${\textbackslash}mathcal\{X\}{\textasciicircum}\{(i)\}\$\$, the i-th component of \$\${\textbackslash}mathcal\{X\}\$\$, would directly follow by composing \$\${\textbackslash}mathcal\{Y\}{\textasciicircum}\{(i)\}\$\$with its cumulative distribution function (c.d.f.) and the inverse c.d.f. of \$\${\textbackslash}mathcal\{X\}{\textasciicircum}\{(i)\}\$\$. In this work, the marginals of \$\${\textbackslash}mathcal\{Y\}\$\$are not explicit, as in a factor copula model. We design an algorithm which samples \$\${\textbackslash}mathcal\{X\}\$\$through an empirical approximation of the c.d.f. of the \$\${\textbackslash}mathcal\{Y\}\$\$-marginals. To be able to handle complex distributions for \$\${\textbackslash}mathcal\{Y\}\$\$or rare-event computations, we allow Markov Chain Monte Carlo (MCMC) samplers. We establish convergence results whose rates depend on the tails of \$\${\textbackslash}mathcal\{X\}\$\$, \$\${\textbackslash}mathcal\{Y\}\$\$and the Lyapunov function of the MCMC sampler. We present numerical experiments confirming the convergence rates and also revisit a real data analysis from financial risk management.},
	language = {en},
	number = {1},
	urldate = {2023-05-16},
	journal = {Methodology and Computing in Applied Probability},
	author = {Bénézet, Cyril and Gobet, Emmanuel and Targino, Rodrigo},
	month = feb,
	year = {2023},
	keywords = {60J22, 62H05, 91G60, Copula model, Markov chain Monte Carlo, Sampling algorithm},
	pages = {13},
	file = {Submitted Version:/Users/jeanpachebat/Zotero/storage/EFYR3LJR/Bénézet et al. - 2023 - Transform MCMC Schemes for Sampling Intractable Fa.pdf:application/pdf},
}

@book{falk_multivariate_2019,
	address = {Cham},
	series = {Springer {Series} in {Operations} {Research} and {Financial} {Engineering}},
	title = {Multivariate {Extreme} {Value} {Theory} and {D}-{Norms}},
	isbn = {978-3-030-03818-2},
	url = {http://link.springer.com/10.1007/978-3-030-03819-9},
	language = {en},
	urldate = {2023-05-16},
	publisher = {Springer International Publishing},
	author = {M. Falk},
	year = {2019},
	doi = {10.1007/978-3-030-03819-9},
	keywords = {Copula, D-Norms, Functional extreme value theory, Multivariate Extreme Value Theory, Multivariate Generalized Pareto Distribution},
	file = {Submitted Version:/Users/jeanpachebat/Zotero/storage/XQV4HTVL/Falk - 2019 - Multivariate Extreme Value Theory and D-Norms.pdf:application/pdf},
}

@article{fort_mcmc_2017,
	title = {{MCMC} design-based non-parametric regression for rare event. {Application} to nested risk computations},
	volume = {23},
	issn = {1569-3961},
	url = {https://www.degruyter.com/document/doi/10.1515/mcma-2017-0101/html},
	doi = {10.1515/mcma-2017-0101},
	abstract = {We design and analyze an algorithm for estimating the mean of a function of a conditional expectation when the outer expectation is related to a rare event. The outer expectation is evaluated through the average along the path of an ergodic Markov chain generated by a Markov chain Monte Carlo sampler. The inner conditional expectation is computed as a non-parametric regression, using a least-squares method with a general function basis and a design given by the sampled Markov chain. We establish non-asymptotic bounds for the L2\$\{L\_\{2\}\}\$-empirical risks associated to this least-squares regression; this generalizes the error bounds usually obtained in the case of i.i.d. observations. Global error bounds are also derived for the nested expectation problem. Numerical results in the context of financial risk computations illustrate the performance of the algorithms.},
	language = {en},
	number = {1},
	urldate = {2023-05-16},
	journal = {Monte Carlo Methods and Applications},
	author = {Fort, Gersende and Gobet, Emmanuel and Moulines, Eric},
	month = mar,
	year = {2017},
	note = {Publisher: De Gruyter},
	keywords = {Empirical regression scheme, MCMC sampler, rare event},
	pages = {21--42},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/W65W7K3M/Fort et al. - 2017 - MCMC design-based non-parametric regression for ra.pdf:application/pdf},
}

@article{falk_new_2021,
	title = {New characterizations of multivariate {Max}-domain of attraction and {D}-{Norms}},
	volume = {24},
	issn = {1386-1999, 1572-915X},
	url = {https://link.springer.com/10.1007/s10687-021-00416-4},
	doi = {10.1007/s10687-021-00416-4},
	abstract = {Abstract
            
              In this paper we derive new results on multivariate extremes and
              D
              -norms. In particular we establish new characterizations of the multivariate max-domain of attraction property. The limit distribution of certain multivariate exceedances above high thresholds is derived, and the distribution of that generator of a
              D
              -norm on
              
                
                  \$\{{\textbackslash}mathbb R\}{\textasciicircum}\{d\}\$
                  
                    
                      
                        ℝ
                      
                      
                        d
                      
                    
                  
                
              
              , whose components sum up to
              d
              , is obtained. Finally we introduce exchangeable
              D
              -norms and show that the set of exchangeable
              D
              -norms is a simplex.},
	language = {en},
	number = {4},
	urldate = {2023-05-16},
	journal = {Extremes},
	author = {M. Falk and T. Fuller},
	year = {2021},
	pages = {849--879},
	file = {Full Text:/Users/jeanpachebat/Zotero/storage/CZFEVCHQ/Falk and Fuller - 2021 - New characterizations of multivariate Max-domain o.pdf:application/pdf},
}

@misc{albergo_building_2023,
	title = {Building {Normalizing} {Flows} with {Stochastic} {Interpolants}},
	url = {http://arxiv.org/abs/2209.15571},
	doi = {10.48550/arXiv.2209.15571},
	abstract = {A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet \$32{\textbackslash}times32\$. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to \$128{\textbackslash}times128\$.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Albergo, Michael S. and Vanden-Eijnden, Eric},
	month = mar,
	year = {2023},
	note = {arXiv:2209.15571 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2023},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/S4NWAPYY/Albergo and Vanden-Eijnden - 2023 - Building Normalizing Flows with Stochastic Interpo.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/6SP45XUB/2209.html:text/html},
}

@misc{li_self-consistent_2023,
	title = {Self-{Consistent} {Velocity} {Matching} of {Probability} {Flows}},
	url = {http://arxiv.org/abs/2301.13737},
	doi = {10.48550/arXiv.2301.13737},
	abstract = {We present a discretization-free scalable framework for solving a large class of mass-conserving partial differential equations (PDEs), including the time-dependent Fokker-Planck equation and the Wasserstein gradient flow. The main observation is that the time-varying velocity field of the PDE solution needs to be self-consistent: it must satisfy a fixed-point equation involving the flow characterized by the same velocity field. By parameterizing the flow as a time-dependent neural network, we propose an end-to-end iterative optimization framework called self-consistent velocity matching to solve this class of PDEs. Compared to existing approaches, our method does not suffer from temporal or spatial discretization, covers a wide range of PDEs, and scales to high dimensions. Experimentally, our method recovers analytical solutions accurately when they are available and achieves comparable or better performance in high dimensions with less training time compared to recent large-scale JKO-based methods that are designed for solving a more restrictive family of PDEs.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Li, Lingxiao and Hurault, Samuel and Solomon, Justin},
	month = feb,
	year = {2023},
	note = {arXiv:2301.13737 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/UGB354UX/Li et al. - 2023 - Self-Consistent Velocity Matching of Probability F.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/YQNQWLVU/2301.html:text/html},
}

@misc{shi_diffusion_2023,
	title = {Diffusion {Schr}{\textbackslash}"odinger {Bridge} {Matching}},
	url = {http://arxiv.org/abs/2303.16852},
	doi = {10.48550/arXiv.2303.16852},
	abstract = {Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schr{\textbackslash}"odinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting, a new methodology for solving SB problems, and Diffusion Schr{\textbackslash}"odinger Bridge Matching (DSBM), a novel numerical algorithm for computing IMF iterates. DSBM significantly improves over previous SB numerics and recovers as special/limiting cases various recent transport methods. We demonstrate the performance of DSBM on a variety of problems.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Shi, Yuyang and De Bortoli, Valentin and Campbell, Andrew and Doucet, Arnaud},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16852 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/CTF4TWA2/Shi et al. - 2023 - Diffusion Schrodinger Bridge Matching.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/MURF48KM/2303.html:text/html},
}

@misc{chen_gradient_2023,
	title = {Gradient {Flows} for {Sampling}: {Mean}-{Field} {Models}, {Gaussian} {Approximations} and {Affine} {Invariance}},
	shorttitle = {Gradient {Flows} for {Sampling}},
	url = {http://arxiv.org/abs/2302.11024},
	doi = {10.48550/arXiv.2302.11024},
	abstract = {Sampling a probability distribution with an unknown normalization constant is a fundamental problem in computational science and engineering. This task may be cast as an optimization problem over all probability measures, and an initial distribution can be evolved to the desired minimizer dynamically via gradient flows. Mean-field models, whose law is governed by the gradient flow in the space of probability measures, may also be identified; particle approximations of these mean-field models form the basis of algorithms. The gradient flow approach is also the basis of algorithms for variational inference, in which the optimization is performed over a parameterized family of probability distributions such as Gaussians, and the underlying gradient flow is restricted to the parameterized family. By choosing different energy functionals and metrics for the gradient flow, different algorithms with different convergence properties arise. In this paper, we concentrate on the Kullback-Leibler divergence after showing that, up to scaling, it has the unique property that the gradient flows resulting from this choice of energy do not depend on the normalization constant. For the metrics, we focus on variants of the Fisher-Rao, Wasserstein, and Stein metrics; we introduce the affine invariance property for gradient flows, and their corresponding mean-field models, determine whether a given metric leads to affine invariance, and modify it to make it affine invariant if it does not. We study the resulting gradient flows in both probability density space and Gaussian space. The flow in the Gaussian space may be understood as a Gaussian approximation of the flow. We demonstrate that the Gaussian approximation based on the metric and through moment closure coincide, establish connections between them, and study their long-time convergence properties showing the advantages of affine invariance.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Chen, Yifan and Huang, Daniel Zhengyu and Huang, Jiaoyang and Reich, Sebastian and Stuart, Andrew M.},
	month = apr,
	year = {2023},
	note = {arXiv:2302.11024 [cs, math, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Numerical Analysis},
	annote = {Comment: 71 pages, 8 figures (Welcome any feedback!)},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/KR5BYTMR/Chen et al. - 2023 - Gradient Flows for Sampling Mean-Field Models, Ga.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/NWT2UJEB/2302.html:text/html},
}

@misc{liu_flow_2022,
	title = {Flow {Straight} and {Fast}: {Learning} to {Generate} and {Transfer} {Data} with {Rectified} {Flow}},
	shorttitle = {Flow {Straight} and {Fast}},
	url = {http://arxiv.org/abs/2209.03003},
	doi = {10.48550/arXiv.2209.03003},
	abstract = {We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions {\textbackslash}pi\_0 and {\textbackslash}pi\_1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from {\textbackslash}pi\_0 and {\textbackslash}pi\_1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of {\textbackslash}pi\_0 and {\textbackslash}pi\_1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
	month = sep,
	year = {2022},
	note = {arXiv:2209.03003 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/SBSYM27V/Liu et al. - 2022 - Flow Straight and Fast Learning to Generate and T.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/W6KAUMXT/2209.html:text/html},
}

@inproceedings{wu_alignment_2022,
	title = {The alignment property of {SGD} noise and how it helps select flat minima: {A} stability analysis},
	shorttitle = {The alignment property of {SGD} noise and how it helps select flat minima},
	url = {https://openreview.net/forum?id=rUc8peDIM45},
	abstract = {The phenomenon that stochastic gradient descent (SGD) favors flat minima has played a critical role in understanding the implicit regularization of SGD. In this paper, we provide an explanation of this striking phenomenon by relating the particular noise structure of SGD to its {\textbackslash}emph\{linear stability\} (Wu et al., 2018). Specifically, we consider training over-parameterized models with square loss. We prove that if a global minimum \${\textbackslash}theta{\textasciicircum}*\$ is linearly stable for SGD, then it must satisfy \${\textbackslash}{\textbar}H({\textbackslash}theta{\textasciicircum}*){\textbackslash}{\textbar}\_F{\textbackslash}leq O({\textbackslash}sqrt\{B\}/{\textbackslash}eta)\$, where \${\textbackslash}{\textbar}H({\textbackslash}theta{\textasciicircum}*){\textbackslash}{\textbar}\_F, B,{\textbackslash}eta\$ denote the Frobenius norm of Hessian at \${\textbackslash}theta{\textasciicircum}*\$, batch size, and learning rate, respectively. Otherwise, SGD will escape from that minimum {\textbackslash}emph\{exponentially\} fast. Hence, for minima accessible to SGD, the sharpness---as measured by the Frobenius norm of the Hessian---is bounded {\textbackslash}emph\{independently\} of the model size and sample size. The key to obtaining these results is exploiting the particular structure of SGD noise: The noise concentrates in sharp directions of local landscape and the magnitude is proportional to loss value. This alignment property of SGD noise provably holds for linear networks and random feature models (RFMs), and is empirically verified for nonlinear networks. Moreover, the validity and practical relevance of our theoretical findings are also justified by extensive experiments on CIFAR-10 dataset.},
	language = {en},
	urldate = {2023-05-16},
	author = {Wu, Lei and Wang, Mingze and Su, Weijie J.},
	month = oct,
	year = {2022},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/62YHZGTN/Wu et al. - 2022 - The alignment property of SGD noise and how it hel.pdf:application/pdf},
}

@misc{asadulaev_neural_2023,
	title = {Neural {Optimal} {Transport} with {General} {Cost} {Functionals}},
	url = {http://arxiv.org/abs/2205.15403},
	doi = {10.48550/arXiv.2205.15403},
	abstract = {Neural optimal transport techniques mostly use Euclidean cost functions, such as \${\textbackslash}ell{\textasciicircum}1\$ or \${\textbackslash}ell{\textasciicircum}2\$. These costs are suitable for translation tasks between related domains, but they are hardly applicable to problems where a specific non-Euclidean optimality of the mapping is required such as dataset transfer. To tackle this issue, we introduce a novel neural network-based algorithm to compute optimal transport plans and maps for general cost functionals. Such functionals provide more flexibility and allow using auxiliary information, such as class labels, to construct the required transport map. Our method is based on a saddle point reformulation of the optimal transport problem and generalizes prior methods for weak and strong transport cost functionals. As an application, we construct a functional to map data distributions with preserving the class-wise structure of data.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Asadulaev, Arip and Korotin, Alexander and Egiazarian, Vage and Mokrov, Petr and Burnaev, Evgeny},
	month = feb,
	year = {2023},
	note = {arXiv:2205.15403 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/EMVQUVFI/Asadulaev et al. - 2023 - Neural Optimal Transport with General Cost Functio.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/MZ9E2SHF/2205.html:text/html},
}

@misc{cai_nf-ula_2023,
	title = {{NF}-{ULA}: {Langevin} {Monte} {Carlo} with {Normalizing} {Flow} {Prior} for {Imaging} {Inverse} {Problems}},
	shorttitle = {{NF}-{ULA}},
	url = {http://arxiv.org/abs/2304.08342},
	doi = {10.48550/arXiv.2304.08342},
	abstract = {Bayesian methods for solving inverse problems are a powerful alternative to classical methods since the Bayesian approach gives a probabilistic description of the problems and offers the ability to quantify the uncertainty in the solution. Meanwhile, solving inverse problems by data-driven techniques also proves to be successful, due to the increasing representation ability of data-based models. In this work, we try to incorporate the data-based models into a class of Langevin-based sampling algorithms in Bayesian inference. Loosely speaking, we introduce NF-ULA (Unadjusted Langevin algorithms by Normalizing Flows), which involves learning a normalizing flow as the prior. In particular, our algorithm only requires a pre-trained normalizing flow, which is independent of the considered inverse problem and the forward operator. We perform theoretical analysis by investigating the well-posedness of the Bayesian solution and the non-asymptotic convergence of the NF-ULA algorithm. The efficacy of the proposed NF-ULA algorithm is demonstrated in various imaging problems, including image deblurring, image inpainting, and limited-angle X-ray computed tomography (CT) reconstruction.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Cai, Ziruo and Tang, Junqi and Mukherjee, Subhadip and Li, Jinglai and Schönlieb, Carola Bibiane and Zhang, Xiaoqun},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08342 [cs, math, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/G6YETJNM/Cai et al. - 2023 - NF-ULA Langevin Monte Carlo with Normalizing Flow.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/4PSLVCGY/2304.html:text/html},
}

@misc{chen_sampling_2023,
	title = {Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
	shorttitle = {Sampling is as easy as learning the score},
	url = {http://arxiv.org/abs/2209.11215},
	doi = {10.48550/arXiv.2209.11215},
	abstract = {We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL\${\textbackslash}cdot\$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an \$L{\textasciicircum}2\$-accurate score estimate (rather than \$L{\textasciicircum}{\textbackslash}infty\$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does not reduce the complexity of SGMs.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Chen, Sitan and Chewi, Sinho and Li, Jerry and Li, Yuanzhi and Salim, Adil and Zhang, Anru R.},
	month = apr,
	year = {2023},
	note = {arXiv:2209.11215 [cs, math, stat]},
	keywords = {Mathematics - Statistics Theory, Computer Science - Machine Learning},
	annote = {Comment: 29 pages},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/BQM7AYPT/Chen et al. - 2023 - Sampling is as easy as learning the score theory .pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/8BNF8H4Y/2209.html:text/html},
}

@misc{lee_convergence_2023,
	title = {Convergence for score-based generative modeling with polynomial complexity},
	url = {http://arxiv.org/abs/2206.06227},
	doi = {10.48550/arXiv.2206.06227},
	abstract = {Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density \$p\$ given a score estimate (an estimate of \${\textbackslash}nabla {\textbackslash}ln p\$) that is accurate in \$L{\textasciicircum}2(p)\$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using annealing to obtain a warm start at each step. Moreover, we show that a predictor-corrector algorithm gives better convergence than using either portion alone.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Lee, Holden and Lu, Jianfeng and Tan, Yixin},
	month = may,
	year = {2023},
	note = {arXiv:2206.06227 [cs, math, stat]},
	keywords = {Mathematics - Statistics Theory, Mathematics - Probability, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 43 pages},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/9PAZM8WN/Lee et al. - 2023 - Convergence for score-based generative modeling wi.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/PCYCWTZD/2206.html:text/html},
}

@article{ivek_reconstruction_2023,
	title = {Reconstruction of incomplete wildfire data using deep generative models},
	volume = {26},
	issn = {1572-915X},
	url = {https://doi.org/10.1007/s10687-022-00459-1},
	doi = {10.1007/s10687-022-00459-1},
	abstract = {We present our submission to the Extreme Value Analysis 2021 Data Challenge in which teams were asked to accurately predict distributions of wildfire frequency and size within spatio-temporal regions of missing data. For this competition, we developed a variant of the powerful variational autoencoder models, which we call Conditional Missing data Importance-Weighted Autoencoder (CMIWAE). Our deep latent variable generative model requires little to no feature engineering and does not necessarily rely on the specifics of scoring in the Data Challenge. It is fully trained on incomplete data, with the single objective to maximize log-likelihood of the observed wildfire information. We mitigate the effects of the relatively low number of training samples by stochastic sampling from a variational latent variable distribution, as well as by ensembling a set of CMIWAE models trained and validated on different splits of the provided data.},
	language = {en},
	number = {2},
	urldate = {2023-05-16},
	journal = {Extremes},
	author = {Ivek, Tomislav and Vlah, Domagoj},
	month = jun,
	year = {2023},
	keywords = {Convolutional neural network, Data reconstruction, Deep learning, Ensemble, Extreme Value Analysis Conference challenge, Variational autoencoder, Wildfires},
	pages = {251--271},
	file = {Submitted Version:/Users/jeanpachebat/Zotero/storage/7BSBDKDW/Ivek and Vlah - 2023 - Reconstruction of incomplete wildfire data using d.pdf:application/pdf},
}

@article{das_tail_2022,
	title = {Tail probabilities of random linear functions of regularly varying random vectors},
	volume = {25},
	issn = {1572-915X},
	url = {https://doi.org/10.1007/s10687-021-00432-4},
	doi = {10.1007/s10687-021-00432-4},
	abstract = {We provide a new extension of Breiman’s Theorem on computing tail probabilities of a product of random variables to a multivariate setting. In particular, we give a characterization of regular variation on cones in \$\$[0,{\textbackslash}infty ){\textasciicircum}d\$\$under random linear transformations. This allows us to compute probabilities of a variety of tail events, which classical multivariate regularly varying models would report to be asymptotically negligible. We illustrate our findings with applications to risk assessment in financial systems and reinsurance markets under a bipartite network structure.},
	language = {en},
	number = {4},
	urldate = {2023-05-16},
	journal = {Extremes},
	author = {Das, Bikramjit and Fasen-Hartmann, Vicky and Klüppelberg, Claudia},
	month = dec,
	year = {2022},
	keywords = {60B10, 60F10, 60G70, 90B15, Bipartite graphs, Heavy-tails, Multivariate regular variation, Networks},
	pages = {721--758},
	file = {Submitted Version:/Users/jeanpachebat/Zotero/storage/E3AS9JEK/Das et al. - 2022 - Tail probabilities of random linear functions of r.pdf:application/pdf},
}

@article{debicki_pandemic-type_2022,
	title = {Pandemic-type failures in multivariate {Brownian} risk models},
	volume = {25},
	issn = {1572-915X},
	url = {https://doi.org/10.1007/s10687-021-00424-4},
	doi = {10.1007/s10687-021-00424-4},
	abstract = {Modelling of multiple simultaneous failures in insurance, finance and other areas of applied probability is important especially from the point of view of pandemic-type events. A benchmark limiting model for the analysis of multiple failures is the classical d-dimensional Brownian risk model (Brm), see Delsing et al. (Methodol. Comput. Appl. Probab. 22(3), 927–948 2020). From both theoretical and practical point of view, of interest is the calculation of the probability of multiple simultaneous failures in a given time horizon. The main findings of this contribution concern the approximation of the probability that at least k out of d components of Brm fail simultaneously. We derive both sharp bounds and asymptotic approximations of the probability of interest for the finite and the infinite time horizon. Our results extend previous findings of Dȩbicki et al. (J. Appl. Probab. 57(2), 597–612 2020) and Dȩbicki et al. (Stoch. Proc. Appl. 128(12), 4171–4206 2018).},
	language = {en},
	number = {1},
	urldate = {2023-05-16},
	journal = {Extremes},
	author = {Dȩbicki, Krzysztof and Hashorva, Enkelejd and Kriukov, Nikolai},
	month = mar,
	year = {2022},
	keywords = {Exact asymptotics, Failure time, Multivariate Brownian risk model, Pandemic-type events, Primary–60G15, Probability of multiple simultaneous failures, Secondary–60G70, Simultaneous ruin probability},
	pages = {1--23},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/BJ7CP2LL/Dȩbicki et al. - 2022 - Pandemic-type failures in multivariate Brownian ri.pdf:application/pdf},
}

@article{ghosal_multivariate_2022,
	title = {Multivariate ranks and quantiles using optimal transport: {Consistency}, rates and nonparametric testing},
	volume = {50},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Multivariate ranks and quantiles using optimal transport},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-2/Multivariate-ranks-and-quantiles-using-optimal-transport--Consistency-rates/10.1214/21-AOS2136.full},
	doi = {10.1214/21-AOS2136},
	abstract = {In this paper, we study multivariate ranks and quantiles, defined using the theory of optimal transport, and build on the work of Chernozhukov et al. (Ann. Statist. 45 (2017) 223–256) and Hallin et al. (Ann. Statist. 49 (2021) 1139–1165). We study the characterization, computation and properties of the multivariate rank and quantile functions and their empirical counterparts. We derive the uniform consistency of these empirical estimates to their population versions, under certain assumptions. In fact, we prove a Glivenko–Cantelli type theorem that shows the asymptotic stability of the empirical rank map in any direction. Under mild structural assumptions, we provide global and local rates of convergence of the empirical quantile and rank maps. We also provide a sub-Gaussian tail bound for the global L2-loss of the empirical quantile function. Further, we propose tuning parameter-free multivariate nonparametric tests—a two-sample test and a test for mutual independence—based on our notion of multivariate quantiles/ranks. Asymptotic consistency of these tests are shown and the rates of convergence of the associated test statistics are derived, both under the null and alternative hypotheses.},
	number = {2},
	urldate = {2023-05-16},
	journal = {The Annals of Statistics},
	author = {Ghosal, Promit and Sen, Bodhisattva},
	month = apr,
	year = {2022},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {35J96, 60F15, 62G20, 62G30, Brenier–McCann’s theorem, convergence of subdifferentials of convex functions, Glivenko–Cantelli type theorem, Legendre–Fenchel dual, local uniform rate of convergence, semidiscrete optimal transport, testing mutual independence, two-sample goodness-of-fit testing},
	pages = {1012--1037},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/KSBASZ9J/Ghosal and Sen - 2022 - Multivariate ranks and quantiles using optimal tra.pdf:application/pdf},
}

@article{einmahl_spatial_2022,
	title = {Spatial dependence and space–time trend in extreme events},
	volume = {50},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-1/Spatial-dependence-and-spacetime-trend-in-extreme-events/10.1214/21-AOS2067.full},
	doi = {10.1214/21-AOS2067},
	abstract = {The statistical theory of extremes is extended to independent multivariate observations that are non-stationary both over time and across space. The non-stationarity over time and space is controlled via the scedasis (tail scale) in the marginal distributions. Spatial dependence stems from multivariate extreme value theory. We establish asymptotic theory for both the weighted sequential tail empirical process and the weighted tail quantile process based on all observations, taken over time and space. The results yield two statistical tests for homoscedasticity in the tail, one in space and one in time. Further, we show that the common extreme value index can be estimated via a pseudo-maximum likelihood procedure based on pooling all (non-stationary and dependent) observations. Our leading example and application is rainfall in Northern Germany.},
	number = {1},
	urldate = {2023-05-16},
	journal = {The Annals of Statistics},
	author = {Einmahl, John H. J. and Ferreira, Ana and Haan, Laurens de and Neves, Cláudia and Zhou, Chen},
	month = feb,
	year = {2022},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G32, 60G70, 62G20, 62G30, 60F17, 62G05, 62G10, Multivariate extreme value statistics, non-identical distributions, sequential tail empirical process, testing},
	pages = {30--52},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/VX6DPHSG/Einmahl et al. - 2022 - Spatial dependence and space–time trend in extreme.pdf:application/pdf},
}

@article{kuchibhotla_least_2022,
	title = {On least squares estimation under heteroscedastic and heavy-tailed errors},
	volume = {50},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-1/On-least-squares-estimation-under-heteroscedastic-and-heavy-tailed-errors/10.1214/21-AOS2105.full},
	doi = {10.1214/21-AOS2105},
	abstract = {We consider least squares estimation in a general nonparametric regression model where the error is allowed to depend on the covariates. The rate of convergence of the least squares estimator (LSE) for the unknown regression function is well studied when the errors are sub-Gaussian. We find upper bounds on the rates of convergence of the LSE when the error has a uniformly bounded conditional variance and has only finitely many moments. Our upper bound on the rate of convergence of the LSE depends on the moment assumptions on the error, the metric entropy of the class of functions involved and the “local” structure of the function class around the truth. We find sufficient conditions on the error distribution under which the rate of the LSE matches the rate of the LSE under sub-Gaussian error. Our results are finite sample and allow for heteroscedastic and heavy-tailed errors.},
	number = {1},
	urldate = {2023-05-16},
	journal = {The Annals of Statistics},
	author = {Kuchibhotla, Arun K. and Patra, Rohit K.},
	month = feb,
	year = {2022},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62E17, 63GO8, Dyadic peeling, finite sample tail probability bounds, heavy tails, interpolation inequality, local envelopes, maximal inequality},
	pages = {277--302},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/FW5QUX4J/Kuchibhotla and Patra - 2022 - On least squares estimation under heteroscedastic .pdf:application/pdf},
}

@article{girard_extreme_2021,
	title = {Extreme conditional expectile estimation in heavy-tailed heteroscedastic regression models},
	volume = {49},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-6/Extreme-conditional-expectile-estimation-in-heavy-tailed-heteroscedastic-regression-models/10.1214/21-AOS2087.full},
	doi = {10.1214/21-AOS2087},
	abstract = {Expectiles define a least squares analogue of quantiles. They have been the focus of a substantial quantity of research in the context of actuarial and financial risk assessment over the last decade. The behaviour and estimation of unconditional extreme expectiles using independent and identically distributed heavy-tailed observations have been investigated in a recent series of papers. We build here a general theory for the estimation of extreme conditional expectiles in heteroscedastic regression models with heavy-tailed noise; our approach is supported by general results of independent interest on residual-based extreme value estimators in heavy-tailed regression models, and is intended to cope with covariates having a large but fixed dimension. We demonstrate how our results can be applied to a wide class of important examples, among which are linear models, single-index models as well as ARMA and GARCH time series models. Our estimators are showcased on a numerical simulation study and on real sets of actuarial and financial data.},
	number = {6},
	urldate = {2023-05-16},
	journal = {The Annals of Statistics},
	author = {Girard, Stéphane and Stupfler, Gilles and Usseglio-Carleve, Antoine},
	month = dec,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G32, 62G20, 62G30, 62G08, expectiles, Extreme value analysis, heavy-tailed distribution, Heteroscedasticity, regression models, residual-based estimators, Single-index model, tail empirical process of residuals},
	pages = {3358--3382},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/JZ5QITNZ/Girard et al. - 2021 - Extreme conditional expectile estimation in heavy-.pdf:application/pdf},
}

@article{daouia_extreme_2023,
	title = {Extreme value modelling of {SARS}-{CoV}-2 community transmission using discrete generalized {Pareto} distributions},
	volume = {10},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220977},
	doi = {10.1098/rsos.220977},
	abstract = {Superspreading has been suggested to be a major driver of overall transmission in the case of SARS-CoV-2. It is, therefore, important to statistically investigate the tail features of superspreading events (SSEs) to better understand virus propagation and control. Our extreme value analysis of different sources of secondary case data indicates that case numbers of SSEs associated with SARS-CoV-2 may be fat-tailed, although substantially less so than predicted recently in the literature, but also less important relative to SSEs associated with SARS-CoV. The results caution against pooling data from both coronaviruses. This could provide policy- and decision-makers with a more reliable assessment of the tail exposure to SARS-CoV-2 contamination. Going further, we consider the broader problem of large community transmission. We study the tail behaviour of SARS-CoV-2 cluster cases documented both in official reports and in the media. Our results suggest that the observed cluster sizes have been fat-tailed in the vast majority of surveyed countries. We also give estimates and confidence intervals of the extreme potential risk for those countries. A key component of our methodology is up-to-date discrete generalized Pareto models which allow for maximum likelihood-based inference of data with a high degree of discreteness.},
	number = {3},
	urldate = {2023-05-16},
	journal = {Royal Society Open Science},
	author = {Daouia, Abdelaati and Stupfler, Gilles and Usseglio-Carleve, Antoine},
	month = mar,
	year = {2023},
	note = {Publisher: Royal Society},
	keywords = {cluster size, COVID-19, discrete extremes, extreme value theory, secondary cases, superspreading},
	pages = {220977},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/ZGJ8AX7I/Daouia et al. - 2023 - Extreme value modelling of SARS-CoV-2 community tr.pdf:application/pdf},
}

@misc{noauthor_extreme_nodate,
	title = {Extreme value modelling of {SARS}-{CoV}-2 community transmission using discrete generalized {Pareto} distributions},
	url = {https://royalsocietypublishing.org/doi/epdf/10.1098/rsos.220977},
	language = {en},
	urldate = {2023-05-16},
	doi = {10.1098/rsos.220977},
	file = {Full Text:/Users/jeanpachebat/Zotero/storage/C4DAHSTA/Extreme value modelling of SARS-CoV-2 community tr.pdf:application/pdf;Snapshot:/Users/jeanpachebat/Zotero/storage/QDQQCHHS/rsos.html:text/html},
}

@misc{biau_optimization_2017,
	title = {Optimization by gradient boosting},
	url = {http://arxiv.org/abs/1707.05023},
	doi = {10.48550/arXiv.1707.05023},
	abstract = {Gradient boosting is a state-of-the-art prediction technique that sequentially produces a model in the form of linear combinations of simple predictors---typically decision trees---by solving an infinite-dimensional convex optimization problem. We provide in the present paper a thorough analysis of two widespread versions of gradient boosting, and introduce a general framework for studying these algorithms from the point of view of functional optimization. We prove their convergence as the number of iterations tends to infinity and highlight the importance of having a strongly convex risk functional to minimize. We also present a reasonable statistical context ensuring consistency properties of the boosting predictors as the sample size grows. In our approach, the optimization procedures are run forever (that is, without resorting to an early stopping strategy), and statistical regularization is basically achieved via an appropriate \$L{\textasciicircum}2\$ penalization of the loss and strong convexity arguments.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Biau, Gérard and Cadre, Benoît},
	month = jul,
	year = {2017},
	note = {arXiv:1707.05023 [cs, math, stat]},
	keywords = {Mathematics - Statistics Theory, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/BLXETWZK/Biau and Cadre - 2017 - Optimization by gradient boosting.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/5D334YE6/1707.html:text/html},
}

@misc{du_wasserstein_2021,
	title = {Wasserstein {Random} {Forests} and {Applications} in {Heterogeneous} {Treatment} {Effects}},
	url = {http://arxiv.org/abs/2006.04709},
	doi = {10.48550/arXiv.2006.04709},
	abstract = {We present new insights into causal inference in the context of Heterogeneous Treatment Effects by proposing natural variants of Random Forests to estimate the key conditional distributions. To achieve this, we recast Breiman's original splitting criterion in terms of Wasserstein distances between empirical measures. This reformulation indicates that Random Forests are well adapted to estimate conditional distributions and provides a natural extension of the algorithm to multivariate outputs. Following the philosophy of Breiman's construction, we propose some variants of the splitting rule that are well-suited to the conditional distribution estimation problem. Some preliminary theoretical connections are established along with various numerical experiments, which show how our approach may help to conduct more transparent causal inference in complex situations.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Du, Qiming and Biau, Gérard and Petit, François and Porcher, Raphaël},
	month = feb,
	year = {2021},
	note = {arXiv:2006.04709 [cs, stat]},
	keywords = {Statistics - Methodology, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/DPVDEQAD/Du et al. - 2021 - Wasserstein Random Forests and Applications in Het.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/R4J7JL23/2006.html:text/html},
}

@misc{biau_accelerated_2018,
	title = {Accelerated {Gradient} {Boosting}},
	url = {http://arxiv.org/abs/1803.02042},
	doi = {10.48550/arXiv.1803.02042},
	abstract = {Gradient tree boosting is a prediction algorithm that sequentially produces a model in the form of linear combinations of decision trees, by solving an infinite-dimensional optimization problem. We combine gradient boosting and Nesterov's accelerated descent to design a new algorithm, which we call AGB (for Accelerated Gradient Boosting). Substantial numerical evidence is provided on both synthetic and real-life data sets to assess the excellent performance of the method in a large variety of prediction problems. It is empirically shown that AGB is much less sensitive to the shrinkage parameter and outputs predictors that are considerably more sparse in the number of trees, while retaining the exceptional performance of gradient boosting.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Biau, Gérard and Cadre, Benoît and Rouvìère, Laurent},
	month = mar,
	year = {2018},
	note = {arXiv:1803.02042 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/4PP6HHJX/Biau et al. - 2018 - Accelerated Gradient Boosting.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/E7Y6CK4Q/1803.html:text/html},
}

@misc{biau_random_2015,
	title = {A {Random} {Forest} {Guided} {Tour}},
	url = {http://arxiv.org/abs/1511.05741},
	doi = {10.48550/arXiv.1511.05741},
	abstract = {The random forest algorithm, proposed by L. Breiman in 2001, has been extremely successful as a general-purpose classification and regression method. The approach, which combines several randomized decision trees and aggregates their predictions by averaging, has shown excellent performance in settings where the number of variables is much larger than the number of observations. Moreover, it is versatile enough to be applied to large-scale problems, is easily adapted to various ad-hoc learning tasks, and returns measures of variable importance. The present article reviews the most recent theoretical and methodological developments for random forests. Emphasis is placed on the mathematical forces driving the algorithm, with special attention given to the selection of parameters, the resampling mechanism, and variable importance measures. This review is intended to provide non-experts easy access to the main ideas.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Biau, Gérard and Scornet, Erwan},
	month = nov,
	year = {2015},
	note = {arXiv:1511.05741 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/V46Z9XQL/Biau and Scornet - 2015 - A Random Forest Guided Tour.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/BGHMF9TS/1511.html:text/html},
}

@article{padoan_joint_2022,
	title = {Joint inference on extreme expectiles for multivariate heavy-tailed distributions},
	volume = {28},
	issn = {1350-7265},
	url = {https://projecteuclid.org/journals/bernoulli/volume-28/issue-2/Joint-inference-on-extreme-expectiles-for-multivariate-heavy-tailed-distributions/10.3150/21-BEJ1375.full},
	doi = {10.3150/21-BEJ1375},
	abstract = {Expectiles induce a law-invariant, coherent and elicitable risk measure that has received substantial attention in actuarial and financial risk management contexts. A number of recent papers have focused on the behaviour and estimation of extreme expectile-based risk measures and their potential for risk assessment was highlighted in financial and actuarial real data applications. Joint inference of several extreme expectiles has however been left untouched; in fact, even the inference about a marginal extreme expectile turns out to be a difficult problem in finite samples, even though an accurate idea of estimation uncertainty is crucial for the construction of confidence intervals in applications to risk management. We investigate the joint estimation of extreme marginal expectiles of a random vector with heavy-tailed marginal distributions, in a general extremal dependence model. We use these results to derive corrected confidence regions for extreme expectiles, as well as a test for the equality of tail expectiles. The methods are showcased in a finite-sample simulation study and on real financial data.},
	number = {2},
	urldate = {2023-05-16},
	journal = {Bernoulli},
	author = {Padoan, Simone A. and Stupfler, Gilles},
	month = may,
	year = {2022},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	keywords = {testing, heavy tails, expectiles, Extremal dependence, joint convergence, joint inference, tail copula},
	pages = {1021--1048},
	file = {Submitted Version:/Users/jeanpachebat/Zotero/storage/JLFQMJ6H/Padoan and Stupfler - 2022 - Joint inference on extreme expectiles for multivar.pdf:application/pdf},
}

@article{noauthor_nonparametric_nodate,
	title = {Nonparametric extreme conditional expectile estimation},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/sjos.12502},
	doi = {10.1111/sjos.12502},
	language = {en},
	urldate = {2023-05-16},
	file = {Snapshot:/Users/jeanpachebat/Zotero/storage/U38GBXYK/sjos.html:text/html;Submitted Version:/Users/jeanpachebat/Zotero/storage/HXLA69DI/Nonparametric extreme conditional expectile estima.pdf:application/pdf},
}

@misc{noauthor_ccsd_nodate,
	title = {{CCSD} {CAS} – {Central} {Authentication} {Service}},
	url = {https://cas.ccsd.cnrs.fr/cas/login?service=https%3A%2F%2Fhal.science%2Fuser%2Flogin%2Fforward-controller%2Fsubmit%2Fforward-action%2Findex},
	urldate = {2023-05-16},
}

@article{allouche_ev-gan_2022,
	title = {{EV}-{GAN}: {Simulation} of extreme events with {ReLU} neural networks},
	volume = {23},
	issn = {1533-7928},
	shorttitle = {{EV}-{GAN}},
	url = {http://jmlr.org/papers/v23/21-0663.html},
	abstract = {Feedforward neural networks based on Rectified linear units (ReLU) cannot efficiently approximate quantile functions which are not bounded, especially in the case of heavy-tailed distributions. We thus propose a new parametrization for the generator of a Generative adversarial network (GAN) adapted to this framework, basing on extreme-value theory. An analysis of the uniform error between the extreme quantile and its GAN approximation is provided: We establish that the rate of convergence of the error is mainly driven by the second-order parameter of the data distribution. The above results are illustrated on simulated data and real financial data. It appears that our approach outperforms the classical GAN in a wide range of situations including high-dimensional and dependent data.},
	number = {150},
	urldate = {2023-05-16},
	journal = {Journal of Machine Learning Research},
	author = {M. Allouche and S. Girard and E. Gobet},
	year = {2022},
	pages = {1--39},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/UI8VQK38/Allouche et al. - 2022 - EV-GAN Simulation of extreme events with ReLU neu.pdf:application/pdf},
}

@article{chernozhukov_mongekantorovich_2017,
	title = {Monge–{Kantorovich} depth, quantiles, ranks and signs},
	volume = {45},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-45/issue-1/MongeKantorovich-depth-quantiles-ranks-and-signs/10.1214/16-AOS1450.full},
	doi = {10.1214/16-AOS1450},
	abstract = {We propose new concepts of statistical depth, multivariate quantiles, vector quantiles and ranks, ranks and signs, based on canonical transportation maps between a distribution of interest on \${\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\$ and a reference distribution on the \$d\$-dimensional unit ball. The new depth concept, called Monge–Kantorovich depth, specializes to halfspace depth for \$d=1\$ and in the case of spherical distributions, but for more general distributions, differs from the latter in the ability for its contours to account for non-convex features of the distribution of interest. We propose empirical counterparts to the population versions of those Monge–Kantorovich depth contours, quantiles, ranks, signs and vector quantiles and ranks, and show their consistency by establishing a uniform convergence property for empirical (forward and reverse) transport maps, which is the main theoretical result of this paper.},
	number = {1},
	urldate = {2023-05-17},
	journal = {The Annals of Statistics},
	author = {Chernozhukov, Victor and Galichon, Alfred and Hallin, Marc and Henry, Marc},
	month = feb,
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G35, 62M15, empirical transport maps, multivariate signs, Statistical depth, uniform convergence of empirical transport, vector quantiles, vector ranks},
	pages = {223--256},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/ZFQWIRFL/Chernozhukov et al. - 2017 - Monge–Kantorovich depth, quantiles, ranks and sign.pdf:application/pdf},
}

@article{hallin_distribution_2021,
	title = {Distribution and quantile functions, ranks and signs in dimension d: {A} measure transportation approach},
	volume = {49},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Distribution and quantile functions, ranks and signs in dimension d},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-2/Distribution-and-quantile-functions-ranks-and-signs-in-dimension-d/10.1214/20-AOS1996.full},
	doi = {10.1214/20-AOS1996},
	abstract = {Unlike the real line, the real space Rd, for d≥2, is not canonically ordered. As a consequence, such fundamental univariate concepts as quantile and distribution functions and their empirical counterparts, involving ranks and signs, do not canonically extend to the multivariate context. Palliating that lack of a canonical ordering has been an open problem for more than half a century, generating an abundant literature and motivating, among others, the development of statistical depth and copula-based methods. We show that, unlike the many definitions proposed in the literature, the measure transportation-based ranks and signs introduced in Chernozhukov, Galichon, Hallin and Henry (Ann. Statist. 45 (2017) 223–256) enjoy all the properties that make univariate ranks a successful tool for semiparametric inference. Related with those ranks, we propose a new center-outward definition of multivariate distribution and quantile functions, along with their empirical counterparts, for which we establish a Glivenko–Cantelli result. Our approach is based on McCann (Duke Math. J. 80 (1995) 309–323) and our results do not require any moment assumptions. The resulting ranks and signs are shown to be strictly distribution-free and essentially maximal ancillary in the sense of Basu (Sankhyā 21 (1959) 247–256) which, in semiparametric models involving noise with unspecified density, can be interpreted as a finite-sample form of semiparametric efficiency. Although constituting a sufficient summary of the sample, empirical center-outward distribution functions are defined at observed values only. A continuous extension to the entire d-dimensional space, yielding smooth empirical quantile contours and sign curves while preserving the essential monotonicity and Glivenko–Cantelli features of the concept, is provided. A numerical study of the resulting empirical quantile contours is conducted.},
	number = {2},
	urldate = {2023-05-17},
	journal = {The Annals of Statistics},
	author = {Hallin, Marc and Barrio, Eustasio del and Cuesta-Albertos, Juan and Matrán, Carlos},
	month = apr,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G30, multivariate signs, 62B05, ancillarity, Basu theorem, cyclical monotonicity, distribution-freeness, Glivenko–Cantelli theorem, multivariate distribution function, Multivariate quantiles, multivariate ranks},
	pages = {1139--1165},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/4LVTFDQR/Hallin et al. - 2021 - Distribution and quantile functions, ranks and sig.pdf:application/pdf},
}

@article{hallin_multivariate_2021,
	title = {Multivariate goodness-of-fit tests based on {Wasserstein} distance},
	volume = {15},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-15/issue-1/Multivariate-goodness-of-fit-tests-based-on-Wasserstein-distance/10.1214/21-EJS1816.full},
	doi = {10.1214/21-EJS1816},
	abstract = {Goodness-of-fit tests based on the empirical Wasserstein distance are proposed for simple and composite null hypotheses involving general multivariate distributions. For group families, the procedure is to be implemented after preliminary reduction of the data via invariance. This property allows for calculation of exact critical values and p-values at finite sample sizes. Applications include testing for location–scale families and testing for families arising from affine transformations, such as elliptical distributions with given standard radial density and unspecified location vector and scatter matrix. A novel test for multivariate normality with unspecified mean vector and covariance matrix arises as a special case. For more general parametric families, we propose a parametric bootstrap procedure to calculate critical values. The lack of asymptotic distribution theory for the empirical Wasserstein distance means that the validity of the parametric bootstrap under the null hypothesis remains a conjecture. Nevertheless, we show that the test is consistent against fixed alternatives. To this end, we prove a uniform law of large numbers for the empirical distribution in Wasserstein distance, where the uniformity is over any class of underlying distributions satisfying a uniform integrability condition but no additional moment assumptions. The calculation of test statistics boils down to solving the well-studied semi-discrete optimal transport problem. Extensive numerical experiments demonstrate the practical feasibility and the excellent performance of the proposed tests for the Wasserstein distance of order p=1 and p=2 and for dimensions at least up to d=5. The simulations also lend support to the conjecture of the asymptotic validity of the parametric bootstrap.},
	number = {1},
	urldate = {2023-05-17},
	journal = {Electronic Journal of Statistics},
	author = {Hallin, Marc and Mordant, Gilles and Segers, Johan},
	month = jan,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {copula, elliptical distribution, Goodness-of-fit, group families, multivariate normality, Optimal transport, semi-discrete problem, skew-t distribution, Wasserstein distance},
	pages = {1328--1371},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/UBICVVYJ/Hallin et al. - 2021 - Multivariate goodness-of-fit tests based on Wasser.pdf:application/pdf},
}

@article{hallin_semi-parametric_2003,
	title = {Semi-parametric efficiency, distribution-freeness and invariance},
	volume = {9},
	issn = {1350-7265},
	url = {https://projecteuclid.org/journals/bernoulli/volume-9/issue-1/Semi-parametric-efficiency-distribution-freeness-and-invariance/10.3150/bj/1068129013.full},
	doi = {10.3150/bj/1068129013},
	abstract = {Semi-parametric models typically involve a finite-dimensional parameter of interest \$\{{\textbackslash}pmb{\textbackslash}theta\}{\textbackslash}in\{{\textbackslash}pmb{\textbackslash}Theta\}{\textbackslash}subseteq{\textbackslash}mathbb\{R\}{\textasciicircum}k\$, along with an infinite-dimensional nuisance parameter{\textasciitilde}\$f\$. Quite often, the submodels corresponding to a fixed value of \$\{{\textbackslash}pmb{\textbackslash}theta\}\$ possess a group structure that induces a maximal invariant \${\textbackslash}sigma\$-field \$\{{\textbackslash}cal B\}(\{{\textbackslash}pmb{\textbackslash}theta\})\$. In classical examples, where \$f\$ denotes the density of some independ{\textbackslash}-ent and identically distributed innovations, \$\{{\textbackslash}cal B\}(\{{\textbackslash}pmb{\textbackslash}theta\})\$ is the \${\textbackslash}sigma\$-field generated by the ranks of the residuals associated with the parameter value \$\{{\textbackslash}pmb{\textbackslash}theta\}\$. It is shown that semi-parametrically efficient distribution-free inference procedures can generally be constructed from parametrically optimal ones by conditioning on \$\{{\textbackslash}cal B\}(\{{\textbackslash}pmb{\textbackslash}theta\})\$; this implies, for instance, that semi-parametric efficiency (at given \${\textbackslash}pmb{\textbackslash}theta\$ and \$f\$) can be attained by means of rank-based methods. The same procedures, when combined with a consistent estimation of the underlying nuis{\textbackslash}-ance density \$f\$, yield conditionally distribution-free semi-parametrically efficient inference methods, for example, semi-parametrically efficient permutation tests. Remarkably, this is achieved without any explicit tangent space or efficient score computations, and without any sample-splitting device. By means of several examples, including both i.i.d. and time-series models, we show how these results apply in models for which rank-based inference or permutation tests have so far seldom been considered.},
	number = {1},
	urldate = {2023-05-17},
	journal = {Bernoulli},
	author = {Hallin, Marc and Werker, Bas J. M.},
	month = feb,
	year = {2003},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	keywords = {distribution-freeness, Adaptiveness, local asymptotic normality, ranks},
	pages = {137--165},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/N984EFSL/Hallin and Werker - 2003 - Semi-parametric efficiency, distribution-freeness .pdf:application/pdf},
}

@article{cordero-erausquin_regularity_2019,
	title = {Regularity of monotone transport maps between unbounded domains},
	volume = {39},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1078-0947},
	url = {https://www.aimsciences.org/en/article/doi/10.3934/dcds.2019297},
	doi = {10.3934/dcds.2019297},
	abstract = {The regularity of monotone transport maps plays an important role in several applications to PDE and geometry. Unfortunately, the classical statements on this subject are restricted to the case when the measures are compactly supported. In this note we show that, in several situations of interest, one can to ensure the regularity of monotone maps even if the measures may have unbounded supports.},
	language = {en},
	number = {12},
	urldate = {2023-05-17},
	journal = {Discrete and Continuous Dynamical Systems},
	author = {Cordero-Erausquin, Dario and Figalli, Alessio},
	month = nov,
	year = {2019},
	note = {Publisher: Discrete and Continuous Dynamical Systems},
	pages = {7101--7112},
	file = {Submitted Version:/Users/jeanpachebat/Zotero/storage/FH2KU6SN/Cordero-Erausquin and Figalli - 2019 - Regularity of monotone transport maps between unbo.pdf:application/pdf},
}

@misc{genevay_learning_2017,
	title = {Learning {Generative} {Models} with {Sinkhorn} {Divergences}},
	url = {http://arxiv.org/abs/1706.00292},
	doi = {10.48550/arXiv.1706.00292},
	abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
	month = oct,
	year = {2017},
	note = {arXiv:1706.00292 [stat]},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/ELCC7H5S/Genevay et al. - 2017 - Learning Generative Models with Sinkhorn Divergenc.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/NDNCQZ44/1706.html:text/html},
}

@misc{lafon_vae_2023,
	title = {A {VAE} approach to sample multivariate extremes},
	url = {https://hal.science/hal-04013214},
	abstract = {Rapidly generating accurate extremes from an observational dataset is crucial when seeking to estimate risks associated with the occurrence of future extremes which could be larger than those already observed. Many applications ranging from the occurrence of natural disasters to financial crashes are involved. This paper details a variational auto-encoder (VAE) approach for sampling multivariate extremes. The proposed architecture is based on the extreme value theory (EVT) and more particularly on the notion of multivariate functions with regular variations. Experiments conducted on synthetic datasets as well as on a dataset of discharge measurements along Danube river network illustrate the relevance of our approach.},
	language = {en},
	urldate = {2023-05-23},
	author = {Lafon, Nicolas and Naveau, Philippe and Fablet, Ronan},
	year = {2023},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/4IGJYLD5/Lafon et al. - 2023 - A VAE approach to sample multivariate extremes.pdf:application/pdf},
}

@article{einmahl_m-estimator_2012,
	title = {An {M}-estimator for tail dependence in arbitrary dimensions},
	volume = {40},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-40/issue-3/An-M-estimator-for-tail-dependence-in-arbitrary-dimensions/10.1214/12-AOS1023.full},
	doi = {10.1214/12-AOS1023},
	abstract = {Consider a random sample in the max-domain of attraction of a multivariate extreme value distribution such that the dependence structure of the attractor belongs to a parametric model. A new estimator for the unknown parameter is defined as the value that minimizes the distance between a vector of weighted integrals of the tail dependence function and their empirical counterparts. The minimization problem has, with probability tending to one, a unique, global solution. The estimator is consistent and asymptotically normal. The spectral measures of the tail dependence models to which the method applies can be discrete or continuous. Examples demonstrate the applicability and the performance of the method.},
	number = {3},
	urldate = {2023-05-23},
	journal = {The Annals of Statistics},
	author = {Einmahl, J. H. J. and Krajina, A. and Segers, J.},
	year = {2012},
	keywords = {62G32, 60G70, 62G20, 60F17, 62G05, 62G10, 60F05, 60K35, Asymptotic statistics, factor model, M-estimation, multivariate extremes, tail dependence},
	pages = {1764--1793},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/XYMN43LV/Einmahl et al. - 2012 - An M-estimator for tail dependence in arbitrary di.pdf:application/pdf},
}

@misc{noauthor_heavy-tail_nodate,
	title = {Heavy-{Tail} {Phenomena}: {Probabilistic} and {Statistical} {Modeling} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/book/10.1007/978-0-387-45024-7},
	urldate = {2023-05-24},
	file = {Heavy-Tail Phenomena\: Probabilistic and Statistical Modeling | SpringerLink:/Users/jeanpachebat/Zotero/storage/VW8LIAYR/978-0-387-45024-7.html:text/html},
}

@book{resnick_heavy-tail_2007,
	address = {New York, N.Y},
	series = {Springer series in operations research and financial engineering},
	title = {Heavy-tail phenomena: probabilistic and statistical modeling},
	isbn = {978-0-387-24272-9 978-0-387-45024-7},
	shorttitle = {Heavy-tail phenomena},
	language = {en},
	publisher = {Springer},
	author = {Resnick, Sidney I.},
	year = {2007},
	keywords = {Extreme value theory, Distribution (Probability theory), Finance, Heavy tail analysis, Mathematical models},
	file = {Resnick - 2007 - Heavy-tail phenomena probabilistic and statistica.pdf:/Users/jeanpachebat/Zotero/storage/V89BUC97/Resnick - 2007 - Heavy-tail phenomena probabilistic and statistica.pdf:application/pdf},
}

@incollection{bradley_financial_2003,
	address = {Amsterdam},
	series = {Handbooks in {Finance}},
	title = {Financial {Risk} and {Heavy} {Tails}},
	volume = {1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444508966500042},
	abstract = {It is of great importance for those in charge of managing risk to understand how financial asset returns are distributed. Practitioners often assume for convenience that the distribu¬tion is normal. Since the 1960s, however, empirical evidence has led many to reject this assumption in favor of various heavy-tailed alternatives. In a heavy-tailed distribution the likelihood that one encounters significant deviations from the mean is much greater than in the case of the normal distribution. It is now commonly accepted that financial asset returns are, in fact, heavy-tailed. The goal of this survey is to examine how these heavy tails affect several aspects of financial portfolio theory and risk management. We describe some of the methods that one can use to deal with heavy tails and we illustrate them using the NASDAQ composite index.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Handbook of {Heavy} {Tailed} {Distributions} in {Finance}},
	publisher = {North-Holland},
	author = {Bradley, Brendan O. and Taqqu, Murad S.},
	editor = {Rachev, Svetlozar T.},
	month = jan,
	year = {2003},
	doi = {10.1016/B978-044450896-6.50004-2},
	pages = {35--103},
	file = {ScienceDirect Snapshot:/Users/jeanpachebat/Zotero/storage/EGCKJKJI/B9780444508966500042.html:text/html;Submitted Version:/Users/jeanpachebat/Zotero/storage/FETJPWT5/Bradley and Taqqu - 2003 - Financial Risk and Heavy Tails.pdf:application/pdf},
}

@article{ressel_homogeneous_2013,
	title = {Homogeneous distributions—{And} a spectral representation of classical mean values and stable tail dependence functions},
	volume = {117},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X13000274},
	doi = {10.1016/j.jmva.2013.02.013},
	abstract = {Homogeneous distributions on R+d and on R¯+d∖︀\{∞¯d\} are shown to be Bauer simplices when normalized. This is used to provide spectral representations for the classical power mean values Mt(x) which turn out to be unique mixtures of the functions x⟼mini≤d(aixi) for t≤1 (with some gaps depending on the dimension d), resp. x⟼maxi≤d(aixi) for t≥1 (without gaps), where ai≥0. There exists a very close connection with so-called stable tail dependence functions of multivariate extreme value distributions. Their characterization by Hofmann (2009) [7] is improved by showing that it is not necessary to assume the triangle inequality — which instead can be deduced.},
	language = {en},
	urldate = {2023-05-24},
	journal = {Journal of Multivariate Analysis},
	author = {Ressel, Paul},
	month = may,
	year = {2013},
	keywords = {Classical mean value, Co-survival function, Fully -increasing, Homogeneous distribution, Spectral representation, Stable tail dependence function},
	pages = {246--256},
	file = {ScienceDirect Full Text PDF:/Users/jeanpachebat/Zotero/storage/JRSWIVZ9/Ressel - 2013 - Homogeneous distributions—And a spectral represent.pdf:application/pdf;ScienceDirect Snapshot:/Users/jeanpachebat/Zotero/storage/HIYMZ8CB/S0047259X13000274.html:text/html},
}

@article{jessen_regularly_nodate,
	title = {{REGULARLY} {VARYING} {FUNCTIONS}},
	abstract = {We consider some elementary functions of the components of a regularly varying random vector such as linear combinations, products, minima, maxima, order statistics, powers. We give conditions under which these functions are again regularly varying, possibly with a diﬀerent index.},
	language = {en},
	journal = {REGULARLY VARYING FUNCTIONS},
	author = {Jessen, Anders Hedegaard and Mikosch, Thomas},
	file = {Jessen and Mikosch - REGULARLY VARYING FUNCTIONS.pdf:/Users/jeanpachebat/Zotero/storage/L786K5NZ/Jessen and Mikosch - REGULARLY VARYING FUNCTIONS.pdf:application/pdf},
}

@article{drees_principal_2021,
	title = {Principal component analysis for multivariate extremes},
	volume = {15},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-15/issue-1/Principal-component-analysis-for-multivariate-extremes/10.1214/21-EJS1803.full},
	doi = {10.1214/21-EJS1803},
	abstract = {In the probabilistic framework of multivariate regular variation, the first order behavior of heavy-tailed random vectors above large radial thresholds is ruled by a homogeneous limit measure. For a high dimensional vector, a reasonable assumption is that the support of this measure is concentrated on a lower dimensional subspace, meaning that certain linear combinations of the components are much likelier to be large than others. Identifying this subspace and thus reducing the dimension will facilitate a refined statistical analysis. In this work we apply Principal Component Analysis (PCA) to a re-scaled version of radially thresholded observations. Within the statistical learning framework of empirical risk minimization, our main focus is to analyze the squared reconstruction error for the exceedances over large radial thresholds. We prove that the empirical risk converges to the true risk, uniformly over all projection subspaces. As a consequence, the best projection subspace is shown to converge in probability to the optimal one, in terms of the Hausdorff distance between their intersections with the unit sphere. In addition, if the exceedances are re-scaled to the unit ball, we obtain finite sample uniform guarantees to the reconstruction error pertaining to the estimated projection subspace. Numerical experiments illustrate the capability of the proposed framework to improve estimators of extreme value parameters.},
	number = {1},
	urldate = {2023-05-25},
	journal = {Electronic Journal of Statistics},
	author = {Drees, Holger and Sabourin, Anne},
	month = jan,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {62G32, 62H25, Dimension reduction, empirical risk minimization, multivariate extreme value analysis, multivariate regular variation, Principal Component Analysis},
	pages = {908--943},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/CBTD3XLU/Drees and Sabourin - 2021 - Principal component analysis for multivariate extr.pdf:application/pdf},
}

@misc{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	doi = {10.48550/arXiv.1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv:1701.07875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeanpachebat/Zotero/storage/JHJ5NZ4Z/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/jeanpachebat/Zotero/storage/Q2F3J755/1701.html:text/html},
}

@article{lenth_practical_1999,
	title = {A {Practical} {Guide} to {Heavy} {Tails}: {Statistical} {Techniques} and {Applications}},
	volume = {94},
	issn = {01621459},
	shorttitle = {A {Practical} {Guide} to {Heavy} {Tails}},
	url = {https://www.jstor.org/stable/2670194?origin=crossref},
	doi = {10.2307/2670194},
	language = {en},
	number = {446},
	urldate = {2023-05-25},
	journal = {Journal of the American Statistical Association},
	author = {Lenth, Russell V. and Alder, Robert J. and Feldman, Raisa E. and Taqqu, Murad S.},
	month = jun,
	year = {1999},
	pages = {653},
	file = {Lenth et al. - 1999 - A Practical Guide to Heavy Tails Statistical Tech.pdf:/Users/jeanpachebat/Zotero/storage/N7LDL3F3/Lenth et al. - 1999 - A Practical Guide to Heavy Tails Statistical Tech.pdf:application/pdf},
}

@book{bingham_regular_1987,
	address = {Cambridge},
	series = {Encyclopedia of {Mathematics} and its {Applications}},
	title = {Regular {Variation}},
	isbn = {978-0-521-37943-4},
	url = {https://www.cambridge.org/core/books/regular-variation/A71D6C4A65CFC1806E1DB7109128A358},
	abstract = {This book is a comprehensive account of the theory and applications of regular variation. It is concerned with the asymptotic behaviour of a real function of a real variable x which is 'close' to a power of x. Such functions are much more than a convenient extension of powers. In many limit theorems regular variation is intrinsic to the result, and exactly characterises the limit behaviour. The book emphasises such characterisations, and gives a comprehensive treatment of those applications where regular variation plays an essential (rather then merely convenient) role. The authors rigorously develop the basic ideas of Karamata theory and de Haan theory including many new results and 'second-order' theorems. They go on to discuss the role of regular variation in Abelian, Tauberian, and Mercerian theorems. These results are then applied in analytic number theory, complex analysis, and probability, with the aim above all of setting the theory in context. A widely scattered literature is thus brought together in a unified approach. With several appendices and a comprehensive list of references, analysts, number theorists, and probabilists will find this an invaluable and complete account of regular variation. It will provide a rigorous and authoritative introduction to the subject for research students in these fields.},
	urldate = {2023-05-25},
	publisher = {Cambridge University Press},
	author = {Bingham, N. H. and Goldie, C. M. and Teugels, J. L.},
	year = {1987},
	doi = {10.1017/CBO9780511721434},
	file = {Snapshot:/Users/jeanpachebat/Zotero/storage/DCCQAZI4/A71D6C4A65CFC1806E1DB7109128A358.html:text/html},
}

@book{beirlant_statistics_2004,
	address = {Hoboken, NJ},
	series = {Wiley series in probability and statistics},
	title = {Statistics of extremes: theory and applications},
    author = {J. Beirlant and Y. Goegebeur and J. Teugels and J. Segers},
	isbn = {978-0-471-97647-9},
	shorttitle = {Statistics of extremes},
	language = {en},
	publisher = {Wiley},
	year = {2004},
	keywords = {Mathematical statistics, Maxima and minima},
	file = {Beirlant - 2004 - Statistics of extremes theory and applications.pdf:/Users/jeanpachebat/Zotero/storage/TL8HT4H4/Beirlant - 2004 - Statistics of extremes theory and applications.pdf:application/pdf},
}
%	editor = {J. Beirlant},


@book{graf_2000,
	address = {Berlin; New York},
	series = {Lecture Notes in Mathematics},
	title = {Foundations of quantization for probability distributions},
	isbn = {978-3-540-67394-1},
	language = {en},
	number = {1730},
	publisher = {Springer},
	author = {S. Graf and H. Luschgy},
	year = {2000},
	keywords = {Distribution (Probability theory), Cluster analysis, Coding theory, Fractals},
	file = {Graf and Luschgy - 2000 - Foundations of quantization for probability distri.pdf:/Users/jeanpachebat/Zotero/storage/UZVZ9DJZ/Graf and Luschgy - 2000 - Foundations of quantization for probability distri.pdf:application/pdf},
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, J. and Weiss, E. and Maheswaranathan, N. and Ganguli, S.},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{papamakarios2021normalizing,
  title={Normalizing flows for probabilistic modeling and inference},
  author={Papamakarios, G. and Nalisnick, E. and Rezende, D. J. and Mohamed, S. and Lakshminarayanan, B.},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={2617--2680},
  year={2021},
  publisher={JMLRORG}
}

@book {asmu:albr:10,
	AUTHOR =	 {S. Asmussen and H. Albrecher},
	TITLE =	 {Ruin probabilities},
	SERIES =	 {Advanced Series on Statistical Science \& Applied
	Probability, 14},
	EDITION =	 {second},
	PUBLISHER =	 {World Scientific Publishing Co. Pte. Ltd.,
	Hackensack, NJ},
	YEAR =	 2010
}

@book {robe:03,
	AUTHOR =	 {P. Robert},
	TITLE =	 {Stochastic networks and queues},
	SERIES =	 {Applications of Mathematics},
	VOLUME =	 52,
	PUBLISHER =	 {Springer-Verlag, Berlin},
	YEAR =	 2003
}

@article {pran:watk:05,
	AUTHOR =	 {Prandini, M. and Watkins, O. J.},
	TITLE =	 {Probabilistic Aircraft Conflict Detection},
	JOURNAL =	 {HYBRIDGE WP3: Reachability analysis for
	probabilistic hybrid systems},
	YEAR =	 2005
}

@article{eba2014guidelines,
	title={Guidelines on the revised common procedures and methodologies for the supervisory review and evaluation process ({SREP}) and supervisory stress testing},
	author={{European Banking Authority}},
	volume={EBA/GL/2014/13},
	JOURNAL={Available at \url{https://eba.europa.eu/regulation-and-policy/supervisory-review-and-evaluation-srep-and-pillar-2/guidelines-for-common-procedures-and-methodologies-for-the-supervisory} \url{-review-and-evaluation-process-srep-and-supervisory-stress-testing}},
	year={2014}
}

@article{ebcBS2023stresstest,
	title={2023 stress test of euro
area banks},
	author={{ECB Banking Supervision}},
	volume={ECB},
	JOURNAL={Available at \url{https://www.bankingsupervision.europa.eu/ecb/pub/pdf/ssm.Report_2023_Stress_Test~96bb5a3af8.en.pdf}},
	year={2023}
}



@inproceedings{goodfellow2014generative,
	title = {Generative {A}dversarial {N}ets},
	author={Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
	booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	pages={2672--2680},
volume = {27},
	year={2014}
}

@article {delmoral2005genealogical,
	AUTHOR = {{Del Moral}, P. and Garnier, J.},
	TITLE = {Genealogical particle analysis of rare events},
	JOURNAL = {The Annals of Applied Probability},
	VOLUME = {15},
	YEAR = {2005},
	NUMBER = {4},
	PAGES={2496--2534}
}
%  (Ann. Appl. Probab.)

@book {bucklew2004introduction,
	AUTHOR = {Bucklew, J. A.},
	TITLE = {Introduction to rare event simulation},
	SERIES = {Springer Series in Statistics},
	PUBLISHER = {Springer-Verlag, New York},
	YEAR = {2004}
}

@inproceedings{kingma2014autoencoding,
	author    = {Kingma, D. P. and Welling, M.},
	title     = {Auto-Encoding Variational {B}ayes},
	booktitle = {2nd International Conference on Learning Representation, {ICLR}},
	year      = {2014},
}

@article {gob:liu:rare:2015,
	AUTHOR = {Gobet, E. and Liu, G.},
	TITLE = {Rare event simulation using reversible shaking transformations},
	JOURNAL = {SIAM Journal on Scientific Computing},
	VOLUME = {37},
	YEAR = {2015},
	NUMBER = {5},
	PAGES = {A2295--A2316}
}
%  (SIAM J. Sci. Comput.)

@article{zscheischler2020typology,
  title={A typology of compound weather and climate events},
  author={Zscheischler, J. and Martius, O. and Westra, S. and Bevacqua, E. and Raymond, C. and Horton, R. M. and van den Hurk, B. and AghaKouchak, A. and J{\'e}z{\'e}quel, A. and Mahecha, M. D. and Maraun, D. and Ramos, A. M. and Ridder, N. N. and Thiery, W. and Vignotto, E.},
  journal={Nature Reviews Earth \& Environment},
  volume={1},
  number={7},
  pages={333--347},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{cremer2022cyber,
  title={Cyber risk and cybersecurity: a systematic review of data availability},
  author={F. Cremer and B. Sheehan and M. Fortmann and A.N. Kia and M. Mullins and F. Murphy and S. Materne},
  journal={The Geneva Papers on risk and insurance-Issues and practice},
  volume={47},
  number={3},
  pages={698--736},
  year={2022},
  publisher={Springer}
}

@book {embrechts2015quantitative,
	AUTHOR = {A.J. McNeil and R. Frey and P. Embrechts},
	TITLE = {Quantitative risk management},
	SERIES = {Princeton Series in Finance},
	PUBLISHER = {Princeton University Press, Princeton, NJ},
	YEAR = {2015},
	PAGES = {xix+699},
	ISBN = {978-0-691-16627-8},
	MRCLASS = {91-02 (62G32 62M10 62P05 91B30 91Gxx)},
	MRNUMBER = {3445371},
}

@book{embklumik1997,
	title={Modelling Extremal Events for Insurance and Finance},
	author={Embrechts, P. and Kl\"uppelberg, C. and Mikosch, T.},
	year={1997},
	publisher={Springer-Verlag, Berlin}
}

@article{boulaguiem2022modeling,
  title={Modeling and simulating spatial extremes by combining extreme value theory with generative adversarial networks},
  author={Boulaguiem, Y. and Zscheischler, J. and Vignotto, E. and van der Wiel, K. and Engelke, S.},
  journal={Environmental Data Science},
  volume={1},
  pages={e5},
  year={2022}
}

@article {feder2020htgan,
	AUTHOR = {Feder, R. M. and Berger, P. and Stein, G.},
	TITLE = {Nonlinear 3{D} cosmic web simulation with heavy-tailed
	generative adversarial networks},
	JOURNAL = {Physical  Review  D.},
	VOLUME = {102},
	YEAR = {2020},
	PAGES = {103504, 18},
	NUMBER = {10},
}

@article{allouche2022estimation,
author={M. Allouche and S. Girard and E. Gobet},
title = "Estimation of extreme quantiles from heavy-tailed distributions with neural networks",
journal="Statistics and Computing",
  Volume={34},
  pages={12},
year="2024"
}

@inproceedings{hasan2022modeling,
  title={Modeling extremes with $ d $-max-decreasing neural networks},
  author={A. Hasan and K. Elkhalil and Y. Ng and J.M. Pereira and S. Farsiu and J. Blanchet and V. Tarokh},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={759--768},
  year={2022},
  organization={PMLR}
}

@article{ng2022inference,
  title={Inference and sampling for archimax copulas},
  author={Ng, Yuting and Hasan, Ali and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17099--17116},
  year={2022}
}

@article{cont2022tail,
  title={Tail-{GAN}: Learning to Simulate Tail Risk Scenarios},
  author={R. Cont and M. Cucuringu and R. Xu and C. Zhang},
  journal={SSRN},
  year={2022}
}

@article{hofert2018hierarchical,
  title={Hierarchical archimax copulas},
  author={M. Hofert  and R. Huser and A. Prasad},
  journal={Journal of Multivariate Analysis},
  volume={167},
  pages={195--211},
  year={2018},
  publisher={Elsevier}
}

@book{joe2014dependence,
  title={Dependence modeling with copulas},
  author={Joe, H.},
  year={2014},
  publisher={CRC press}
}

@inproceedings{liang_fattailed_2022,
	title = {Fat–{Tailed} {Variational} {Inference} with {Anisotropic} {Tail} {Adaptive} {Flows}},
	url = {https://proceedings.mlr.press/v162/liang22a.html},
	abstract = {While fat-tailed densities commonly arise as posterior and marginal distributions in robust models and scale mixtures, they present a problematic scenario when Gaussian-based variational inference fails to accurately capture tail decay. We first improve previous theory on tails of Lipschitz flows by quantifying how they affect the rate of tail decay and expanding the theory to non-Lipschitz polynomial flows. Next, we develop an alternative theory for multivariate tail parameters which is sensitive to tail-anisotropy. In doing so, we unveil a fundamental problem which plagues many existing flow-based methods: they can only model tail-isotropic distributions (i.e., distributions having the same tail parameter in every direction). To mitigate this and enable modeling of tail-anisotropic targets, we propose anisotropic tail-adaptive flows (ATAF). Experimental results confirm ATAF on both synthetic and real-world targets is competitive with prior work while also exhibiting appropriate tail-anisotropy.},
	language = {en},
	urldate = {2024-01-11},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {F. Liang and M. Mahoney and L. Hodgkinson},
	year = {2022},
	pages = {13257--13270},
	file = {Full Text PDF:/Users/jeanpachebat/Zotero/storage/VQNKMQXC/Liang et al. - 2022 - Fat–Tailed Variational Inference with Anisotropic .pdf:application/pdf},
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

 @book{murpy_2022,
 author = {K. P. Murphy},
 title = "Probabilistic Machine Learning: An introduction",
 publisher = "MIT Press",
 year = {2022},
 url = "probml.ai"}

  @article{Gnedenko_1943,
 ISSN = {0003486X, 19398980},
 URL = {http://www.jstor.org/stable/1968974},
 author = {B. Gnedenko},
 journal = {Annals of Mathematics},
 number = {3},
 pages = {423--453},
 publisher = {[Annals of Mathematics, Trustees of Princeton University on Behalf of the Annals of Mathematics, Mathematics Department, Princeton University]},
 title = {Sur La Distribution Limite Du Terme Maximum D'Une Série Aléatoire},
 urldate = {2024-06-25},
 volume = {44},
 year = {1943}
}

@article{Fisher_Tippett_1928,
title={Limiting forms of the frequency distribution of the largest or smallest member of a sample},
volume={24},
  DOI={10.1017/S0305004100015681},
  number={2},
  journal={Mathematical Proceedings of the Cambridge Philosophical Society},
  author={Fisher, R. A. and Tippett, L. H. C.},
  year={1928},
  pages={180--190}
}

@article{wu_hyperparameter_2019,
	title = {Hyperparameter {Optimization} for {Machine} {Learning} {Models} {Based} on {Bayesian} {Optimization}},
	volume = {17},
	abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.},
	language = {en},
	number = {1},
    pages = {26--40},
	author = {Wu, J. and Chen, X.-Y. and Zhang, H. and Xiong, L.-D. and Lei, H. and Deng, S.-H.},
	year = {2019},
	file = {Wu et al. - 2019 - Hyperparameter Optimization for Machine Learning M.pdf:/Users/jeanpachebat/Zotero/storage/UQMSYJMC/Wu et al. - 2019 - Hyperparameter Optimization for Machine Learning M.pdf:application/pdf},
}

@misc{hickling_flexible_2024,
	title = {Flexible {Tails} for {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/2406.16971},
	abstract = {Normalizing flows are a flexible class of probability distributions, expressed as transformations of a simple base distribution. A limitation of standard normalizing flows is representing distributions with heavy tails, which arise in applications to both density estimation and variational inference. A popular current solution to this problem is to use a heavy tailed base distribution. Examples include the tail adaptive flow (TAF) methods of Laszkiewicz et al. (2022). We argue this can lead to poor performance due to the difficulty of optimising neural networks, such as normalizing flows, under heavy tailed input. This problem is demonstrated in our paper. We propose an alternative: use a Gaussian base distribution and a final transformation layer which can produce heavy tails. We call this approach tail transform flow (TTF). Experimental results show this approach outperforms current methods, especially when the target distribution has large dimension or tail weight.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {T. Hickling and D. Prangle},
	year = {2024},
	note = {arXiv:2406.16971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Hickling_Prangle_2024_Flexible Tails for Normalizing Flows.pdf:/Users/jeanpachebat/Library/Mobile Documents/com~apple~CloudDocs/zoterodb/Hickling_Prangle_2024_Flexible Tails for Normalizing Flows.pdf:application/pdf},
}

@article{sklar_fonctions_nodate,
	title = {Fonctions de répartition à {$N$} dimensions et leurs marges},
	language = {fr},
	author = {Sklar, M.},
        journal = {Publications de l’Institut Statistique de l’Université de Paris},
        number = {8},
        pages = {229--231},
    year= 1959,
	file = {Sklar - Fonctions de répartition à N dimensions et leurs m.pdf:/Users/jeanpachebat/Zotero/storage/6YZQDENH/Sklar - Fonctions de répartition à N dimensions et leurs m.pdf:application/pdf},
}



@book{nelsen_introduction_2006,
	address = {New York Berlin Heidelberg},
	edition = {second},
	series = {Springer series in statistics},
	title = {An introduction to copulas},
	isbn = {978-0-387-28659-4},
	language = {en},
	publisher = {Springer},
	author = {R.B. Nelsen},
	year = {2006},
	file = {Nelsen_2006_An introduction to copulas.pdf:/Users/jeanpachebat/Library/Mobile Documents/com~apple~CloudDocs/zoterodb/Nelsen_2006_An introduction to copulas.pdf:application/pdf},
}

@misc{kingma_adam_2017,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, D. P. and Ba, J.},
  note={arXiv:1412.6980},
  year={2014}
}


@article{maas_rectier_nodate,
	title = {Rectiﬁer {Nonlinearities} {Improve} {Neural} {Network} {Acoustic} {Models}},
	abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectiﬁed linear (ReL) hidden units demonstrates additional gains in ﬁnal system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectiﬁer networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectiﬁer nonlinearities produce 2\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify diﬀerences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectiﬁer networks.},
	language = {en},
	journal = {Proceedings of the International Conference on Machine Learning (ICML)},
        year = {2013},
	author = {A.L. Maas and A.Y. Hannun and A.Y. Ng},
	file = {Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf:/Users/jeanpachebat/Zotero/storage/I4UGQTLG/Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf:application/pdf},
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C.},
  volume={338},
  year={2009},
  publisher={Springer}
}

@phdthesis{Nadjahi2021,
  TITLE = {{Sliced-Wasserstein distance for large-scale machine learning : theory, methodology and extensions}},
  AUTHOR = {Nadjahi, K.},
  URL = {https://theses.hal.science/tel-03533097},
  NUMBER = {2021IPPAT050},
  SCHOOL = {{Institut Polytechnique de Paris}},
  YEAR = {2021},
  MONTH = Nov,
  KEYWORDS = {Machine learning ; Optimal transport ; Generative modeling ; Apprentissage automatique ; Transport optimal ; Mod{\'e}lisation g{\'e}n{\'e}rative},
  TYPE = {{PhD thesis}},
  PDF = {https://theses.hal.science/tel-03533097/file/106842_NADJAHI_2021_archivage.pdf},
  HAL_ID = {tel-03533097},
  HAL_VERSION = {v1},
}@inproceedings{ajay2023,
  title = {Is conditional generative modeling all you need for decision making?},
  booktitle = {The 11th international conference on learning representations},
  author = {Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua B. and Jaakkola, Tommi S. and Agrawal, Pulkit},
  year = 2023,
  url = {https://openreview.net/forum?id=sP1fo2K9DFG},
  note = {Comment: Website: https://anuragajay.github.io/decision-diffuser/}
}

@misc{akiba2019,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = 2019,
  month = jul,
  number = {arXiv:1907.10902},
  eprint = {1907.10902},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2025-08-03},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 10 pages, Accepted at KDD 2019 Applied Data Science track}
}

@article{allouche2025,
  title = {Learning extreme expected shortfall and conditional tail moments with neural networks. {{Application}} to cryptocurrency data},
  author = {Allouche, Micha{\"e}l and Girard, St{\'e}phane and Gobet, Emmanuel},
  year = 2025,
  month = feb,
  journal = {Neural Networks},
  volume = {182},
  pages = {106903},
  issn = {08936080},
  doi = {10.1016/j.neunet.2024.106903},
  urldate = {2025-02-21},
  abstract = {We propose a neural networks method to estimate extreme Expected Shortfall, and even more generally, extreme conditional tail moments as functions of confidence levels, in heavy-tailed settings. The convergence rate of the uniform error between the log-conditional tail moment and its neural network approximation is established leveraging extreme-value theory (in particular the high-order condition on the distribution tails) and using critically two activation functions (eLU and ReLU) for neural networks. The finite sample performance of the neural network estimator is compared to bias-reduced extreme-value competitors using synthetic heavy-tailed data. The experiments reveal that our method largely outperforms others. In addition, the selection of the anchor point appears to be much easier and stabler than for other methods. Finally, the neural network estimator is tested on real data related to extreme loss returns in cryptocurrencies: here again, the accuracy obtained by cross-validation is excellent, and is much better compared with competitors.},
  langid = {english}
}

@article{banner2008,
  title = {Local times of ranked continuous semimartingales},
  author = {Banner, Adrian D. and Ghomrasni, Raouf},
  year = 2008,
  month = jul,
  journal = {Stochastic Processes and their Applications},
  volume = {118},
  number = {7},
  pages = {1244--1253},
  issn = {03044149},
  doi = {10.1016/j.spa.2007.08.001},
  urldate = {2024-12-12},
  abstract = {Given a finite collection of continuous semimartingales, we derive a semimartingale decomposition of the corresponding ranked (order-statistics) processes. We apply the decomposition to extend the theory of equity portfolios generated by ranked market weights to the case where the stock values admit triple points. c 2007 Elsevier B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@article{barrera2025,
  title = {Statistical {{Learning}} of {{Value-at-Risk}} and {{Expected Shortfall}}},
  author = {Barrera, D. and Cr{\'e}pey, S. and Gobet, E. and Nguyen, Hoang-Dung and Saadeddine, B.},
  year = 2025,
  journal = {Mathematical Finance, to appear},
  eprint = {2209.06476},
  primaryclass = {q-fin, stat},
  url = {http://arxiv.org/abs/2209.06476},
  urldate = {2024-09-20},
  abstract = {We propose a non-asymptotic convergence analysis of a two-step approach to learn a conditional value-at-risk (VaR) and a conditional expected shortfall (ES) using Rademacher bounds, in a non-parametric setup allowing for heavy-tails on the financial loss. Our approach for the VaR is extended to the problem of learning at once multiple VaRs corresponding to different quantile levels. This results in efficient learning schemes based on neural network quantile and least-squares regressions. An a posteriori Monte Carlo procedure is introduced to estimate distances to the ground-truth VaR and ES. This is illustrated by numerical experiments in a Student-t toy model and a financial case study where the objective is to learn a dynamic initial margin.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Finance - Computational Finance,Statistics - Machine Learning}
}

@misc{chen2025,
  title = {Diffusion {{Factor Models}}: {{Generating High-Dimensional Returns}} with {{Factor Structure}}},
  shorttitle = {Diffusion {{Factor Models}}},
  author = {Chen, Minshuo and Xu, Renyuan and Xu, Yumin and Zhang, Ruixun},
  year = 2025,
  month = jul,
  number = {arXiv:2504.06566},
  eprint = {2504.06566},
  primaryclass = {q-fin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.06566},
  urldate = {2025-09-01},
  abstract = {Financial scenario simulation is essential for risk management and portfolio optimization, yet it remains challenging especially in high-dimensional and small data settings common in finance. We propose a diffusion factor model that integrates latent factor structure into generative diffusion processes, bridging econometrics with modern generative AI to address the challenges of the curse of dimensionality and data scarcity in financial simulation. By exploiting the low-dimensional factor structure inherent in asset returns, we decompose the score function--a key component in diffusion models--using time-varying orthogonal projections, and this decomposition is incorporated into the design of neural network architectures. We derive rigorous statistical guarantees, establishing nonasymptotic error bounds for both score estimation at O(d{\textasciicircum}\{5/2\} n{\textasciicircum}\{-2/(k+5)\}) and generated distribution at O(d{\textasciicircum}\{5/4\} n{\textasciicircum}\{-1/2(k+5)\}), primarily driven by the intrinsic factor dimension k rather than the number of assets d, surpassing the dimension-dependent limits in the classical nonparametric statistics literature and making the framework viable for markets with thousands of assets. Numerical studies confirm superior performance in latent subspace recovery under small data regimes. Empirical analysis demonstrates the economic significance of our framework in constructing mean-variance optimal portfolios and factor portfolios. This work presents the first theoretical integration of factor structure with diffusion models, offering a principled approach for high-dimensional financial simulation with limited data. Our code is available at https://github.com/xymmmm00/diffusion\_factor\_model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Mathematical Finance,Quantitative Finance - Statistical Finance}
}

@incollection{david1998,
  title = {Concomitants of order statistics},
  booktitle = {Order statistics: {{Theory}} \& methods},
  author = {David, H.A. and Nagaraja, H.N.},
  year = 1998,
  series = {Handbook of statistics},
  volume = {16},
  pages = {487--513},
  publisher = {Elsevier},
  issn = {0169-7161},
  doi = {10.1016/S0169-7161(98)16020-0}
}

@book{david2004,
  title = {Order statistics},
  author = {David, H.A. and Nagaraja, H.N.},
  year = 2004,
  series = {Wiley series in probability and statistics},
  publisher = {Wiley},
  url = {https://books.google.fr/books?id=bdhzFXg6xFkC},
  isbn = {978-0-471-65401-8}
}

@article{davidherberta1973,
  title = {Concomitants of order statistics},
  author = {{David, Herbert A}},
  year = 1973,
  journal = {Bull. Inst. Internat. Statist.},
  volume = {45},
  number = {1},
  pages = {295--300}
}

@misc{europeancommission2024,
  author = {{European Commission}},
  year = 2024,
  url = {https://finance.ec.europa.eu/sustainable-finance/tools-and-standards/esg-rating-activities_en}
}

@misc{europeancommission2024a,
  author = {{European Commission}},
  year = 2024,
  url = {https://finance.ec.europa.eu/sustainable-finance/overview-sustainable-finance_en}
}

@book{glasserman2013,
  title = {Monte carlo methods in financial engineering},
  author = {Glasserman, P.},
  year = 2013,
  series = {Stochastic modelling and applied probability},
  publisher = {Springer New York},
  url = {https://books.google.fr/books?id=aeAlBQAAQBAJ},
  isbn = {978-0-387-21617-1}
}

@article{haario1999,
  title = {Adaptive proposal distribution for random walk {{Metropolis}} algorithm},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  year = 1999,
  month = sep,
  journal = {Computational Statistics},
  volume = {14},
  number = {3},
  pages = {375--395},
  issn = {0943-4062, 1613-9658},
  doi = {10.1007/s001800050022},
  urldate = {2025-07-08},
  abstract = {The choice of a suitable MCMC method and further the choice of a proposal distribution is known to be crucial for the convergence of the Markov chain. However, in many cases the choice of an effective proposal distribution is difficult. As a remedy we suggest a method called Adaptive Proposal (AP). Although the stationary distribution of the AP algorithm is slightly biased, it appears to provide an efficient tool for, e.g., reasonably low dimensional problems, as typically encountered in non-linear regression problems in natural sciences. As a realistic example we include a successful application of the AP algorithm in parameter estimation for the satellite instrument 'GOMOS'. In this paper we also present systematic performance criteria for comparing Adaptive Proposal algorithm with more traditional Metropolis algorithms.},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@book{kallenberg2002,
  title = {Foundations of modern probability},
  author = {Kallenberg, O.},
  year = 2002,
  series = {Probability and its applications},
  publisher = {Springer New York},
  url = {https://books.google.fr/books?id=L6fhXh13OyMC},
  isbn = {978-0-387-95313-7},
  lccn = {2001032816}
}

@article{kim1990,
  title = {On the dependence structure of order statistics and concomitants of order statistics},
  author = {Kim, S.H. and David, H.A.},
  year = 1990,
  month = mar,
  journal = {Journal of Statistical Planning and Inference},
  volume = {24},
  number = {3},
  pages = {363--368},
  publisher = {Elsevier BV},
  issn = {0378-3758},
  doi = {10.1016/0378-3758(90)90055-y},
  urldate = {2025-07-21},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@article{ledford1998,
  title = {Concomitant tail behaviour for extremes},
  author = {Ledford, Anthony W. and Tawn, Jonathan A.},
  year = 1998,
  journal = {Advances in Applied Probability},
  volume = {30},
  number = {1},
  pages = {197--215},
  doi = {10.1239/aap/1035228000}
}

@article{lee1999a,
  title = {The joint covariance structure of ordered symmetrically dependent observations and their concomitants of order statistics},
  author = {Lee, Hak-Myung and Viana, Marlos},
  year = 1999,
  month = jul,
  journal = {Statistics \& Probability Letters},
  volume = {43},
  number = {4},
  pages = {411--414},
  issn = {01677152},
  doi = {10.1016/S0167-7152(98)00281-8},
  urldate = {2025-07-16},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@misc{liu2025,
  title = {Error {{Correcting Codes}} for {{Segmented Burst-Deletion Channels}}},
  author = {Liu, Yajuan and Duman, Tolga M.},
  year = 2025,
  month = jul,
  number = {arXiv:2507.14070},
  eprint = {2507.14070},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2025-07-21},
  abstract = {We study segmented burst-deletion channels motivated by the observation that synchronization errors commonly occur in a bursty manner in real-world settings. In this channel model, transmitted sequences are implicitly divided into non-overlapping segments, each of which may experience at most one burst of deletions. In this paper, we develop error correction codes for segmented burst-deletion channels over arbitrary alphabets under the assumption that each segment may contain only one burst of t-deletions. The main idea is to encode the input subsequence corresponding to each segment using existing one-burst deletion codes, with additional constraints that enable the decoder to identify segment boundaries during the decoding process from the received sequence. The resulting codes achieve redundancy that scales as O(log b), where b is the length of each segment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Mathematics - Information Theory}
}

@article{nagaraja1994,
  title = {Distribution of the {{Maximum}} of {{Concomitants}} of {{Selected Order Statistics}}},
  author = {Nagaraja, H. N. and David, H. A.},
  year = 1994,
  month = mar,
  journal = {The Annals of Statistics},
  volume = {22},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  doi = {10.1214/aos/1176325380},
  urldate = {2025-07-21}
}

@inproceedings{ng2022a,
  title = {Inference and sampling for archimax copulas},
  booktitle = {Proceedings of the 36th international conference on neural information processing systems},
  author = {Ng, Yuting and Hasan, Ali and Tarokh, Vahid},
  year = 2022,
  abstract = {Understanding multivariate dependencies in both the bulk and the tails of a distribution is an important problem for many applications, such as ensuring algorithms are robust to observations that are infrequent but have devastating effects. Archimax copulas are a family of distributions endowed with a precise representation that allows simultaneous modeling of the bulk and the tails of a distribution. Rather than separating the two as is typically done in practice, incorporating additional information from the bulk may improve inference of the tails, where observations are limited. Building on the stochastic representation of Archimax copulas, we develop a non-parametric inference method and sampling algorithm. Our proposed methods, to the best of our knowledge, are the first that allow for highly flexible and scalable inference and sampling algorithms, enabling the increased use of Archimax copulas in practical settings. We experimentally compare to state-of-the-art density modeling techniques, and the results suggest that the proposed method effectively extrapolates to the tails while scaling to higher dimensional data. Our findings suggest that the proposed algorithms can be used in a variety of applications where understanding the interplay between the bulk and the tails of a distribution is necessary, such as healthcare and safety.},
  isbn = {978-1-7138-7108-8},
  note = {Comment: Yuting Ng and Ali Hasan contributed equally to this work. This work has been accepted at NeurIPS 2022}
}

@article{pastor2022,
  title = {Dissecting green returns},
  author = {P{\'a}stor, {\v L}ubo{\v s} and Stambaugh, Robert F. and Taylor, Lucian A.},
  year = 2022,
  month = nov,
  journal = {Journal of Financial Economics},
  volume = {146},
  number = {2},
  pages = {403--424},
  issn = {0304405X},
  doi = {10.1016/j.jfineco.2022.07.007},
  urldate = {2024-10-16},
  langid = {english}
}

@inproceedings{prillo2020,
  title = {{{SoftSort}}: a continuous relaxation for the argsort operator},
  booktitle = {Proceedings of the 37th international conference on machine learning},
  author = {Prillo, Sebastian and Eisenschlos, Julian},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = 2020,
  month = jul,
  series = {Proceedings of machine learning research},
  volume = {119},
  pages = {7793--7802},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v119/prillo20a.html},
  abstract = {While sorting is an important procedure in computer science, the argsort operator - which takes as input a vector and returns its sorting permutation - has a discrete image and thus zero gradients almost everywhere. This prohibits end-to-end, gradient-based learning of models that rely on the argsort operator. A natural way to overcome this problem is to replace the argsort operator with a continuous relaxation. Recent work has shown a number of ways to do this, but the relaxations proposed so far are computationally complex. In this work we propose a simple continuous relaxation for the argsort operator which has the following qualities: it can be implemented in three lines of code, achieves state-of-the-art performance, is easy to reason about mathematically - substantially simplifying proofs - and is faster than competing approaches. We open source the code to reproduce all of the experiments and results.},
  note = {Comment: 9 pages, 6 figures. Accepted at ICML 2020}
}

@article{rosenblatt1952,
  title = {Remarks on a {{Multivariate Transformation}}},
  author = {Rosenblatt, Murray},
  year = 1952,
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {3},
  pages = {470--472},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177729394},
  urldate = {2025-07-21},
  langid = {english}
}

@article{vanderbeck2021,
  title = {Flow-{{Driven ESG Returns}}},
  author = {Van Der Beck, Philippe},
  year = 2021,
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3929359},
  urldate = {2025-01-20},
  abstract = {I show that the recent returns to ESG investing are strongly driven by price impact from flows towards ESG portfolios. Using data on trades, I estimate the market's ability to accommodate ESG flows, which is given by the elasticity of substitution between ESG and other stocks. I show that every dollar flowing towards a representative ESG portfolio increases the market value of ESG stocks by \$0.5. Using a new measure of total ESG flows, I estimate an annual flow-driven ESG return of 2.5\%. In the absence of flows, ESG stocks would not have outperformed the market from 2012 to 2024.},
  langid = {english}
}

@article{wang2009,
  title = {Concomitants of order statistics for dependent samples},
  author = {Wang, Ke and Nagaraja, H.N.},
  year = 2009,
  month = feb,
  journal = {Statistics \& Probability Letters},
  volume = {79},
  number = {4},
  pages = {553--558},
  publisher = {Elsevier BV},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2008.10.004},
  urldate = {2025-07-21},
  abstract = {We study the distributions of Y -concomitants of the X -order statistics for a special dependent sample (Xi, Yi), i = 1, . . . , n. The dependence among the sample is due to the Xi's, which are assumed to be distributed as equally-correlated multivariate normal. The finite-sample and asymptotic distributions of concomitants are derived under this setup.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@inproceedings{wei2018,
  title = {Improving the improved training of wasserstein {{GANs}}},
  booktitle = {International conference on learning representations},
  author = {Wei, Xiang and Liu, Zixia and Wang, Liqiang and Gong, Boqing},
  year = 2018,
  url = {https://openreview.net/forum?id=SJx9GQb0-},
  note = {Comment: NIPS camera-ready}
}

@article{yang1977,
  title = {General {{Distribution Theory}} of the {{Concomitants}} of {{Order Statistics}}},
  author = {Yang, S. S.},
  year = 1977,
  month = sep,
  journal = {The Annals of Statistics},
  volume = {5},
  number = {5},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  doi = {10.1214/aos/1176343954},
  urldate = {2025-07-21}
}

@article{yao2024,
  title = {Generative adversarial ranking nets},
  author = {Yao, Yinghua and Pan, Yuangang and Li, Jing and Tsang, Ivor W. and Yao, Xin},
  year = 2024,
  journal = {Journal of Machine Learning Research},
  volume = {25},
  number = {119},
  pages = {1--35},
  url = {http://jmlr.org/papers/v25/23-0461.html}
}

@misc{albergo2023b,
  title = {Building {{Normalizing Flows}} with {{Stochastic Interpolants}}},
  author = {Albergo, Michael S. and {Vanden-Eijnden}, Eric},
  year = 2023,
  month = mar,
  number = {arXiv:2209.15571},
  eprint = {2209.15571},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.15571},
  urldate = {2025-09-22},
  abstract = {A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet \$32\textbackslash times32\$. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to \$128\textbackslash times128\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{allouche2024b,
  title = {Statistical error bounds for weighted mean and median, with application to robust aggregation of cryptocurrency data},
  author = {Allouche, Micha{\"e}l and Echenim, Mnacho and Gobet, Emmanuel and Maurice, Anne-Claire},
  year = 2024,
  month = dec,
  urldate = {2025-09-25},
  abstract = {We study price aggregation methodologies applied to crypto-currency prices with quotations fragmented on different platforms. An intrinsic difficulty is that the price returns and volumes are heavytailed, with many outliers, making averaging and aggregation challenging. While conventional methods rely on Volume-Weighted Average Prices (called VWAPs), or Volume-Weighted Median prices (called VWMs), we develop a new Robust Weighted Median (RWM) estimator that is robust to price and volume outliers. Our study is based on new probabilistic concentration inequalities for weighted means and weighted quantiles under different tail assumptions (heavy tails, sub-gamma tails, sub-Gaussian tails). This justifies that fluctuations of VWAP and VWM are statistically important given the heavy-tailed properties of volumes and/or prices. We show that our RWM estimator overcomes this problem and also satisfies all the desirable properties of a price aggregator. We illustrate the behavior of RWM on synthetic data (within a parametric model close to real data): our estimator achieves a statistical accuracy twice as good as its competitors, and also allows to recover realized volatilities in a very accurate way. Tests on real data are also performed and confirm the good behavior of the estimator on various use cases.},
  keywords = {concentration inequalities,heavy tails,outliers,robust aggregation,weighted mean and quantile estimation}
}

@misc{anil2025,
  title = {Fine-{{Tuning Diffusion Models}} via {{Intermediate Distribution Shaping}}},
  author = {Anil, Gautham Govind and Haque, Shaan Ul and Kannen, Nithish and Nagaraj, Dheeraj and Shakkottai, Sanjay and Shanmugam, Karthikeyan},
  year = 2025,
  month = oct,
  number = {arXiv:2510.02692},
  eprint = {2510.02692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.02692},
  urldate = {2025-11-05},
  abstract = {Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an \$8.81\textbackslash\%\$ relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{black2024a,
  title = {Training {{Diffusion Models}} with {{Reinforcement Learning}}},
  author = {Black, Kevin and Janner, Michael and Du, Yilun and Kostrikov, Ilya and Levine, Sergey},
  year = 2024,
  month = jan,
  number = {arXiv:2305.13301},
  eprint = {2305.13301},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.13301},
  urldate = {2025-09-18},
  abstract = {Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{bruna2024,
  title = {Posterior {{Sampling}} with {{Denoising Oracles}} via {{Tilted Transport}}},
  author = {Bruna, Joan and Han, Jiequn},
  year = 2024,
  month = jun,
  number = {arXiv:2407.00745},
  eprint = {2407.00745},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.00745},
  urldate = {2025-09-18},
  abstract = {Score-based diffusion models have significantly advanced high-dimensional data generation across various domains, by learning a denoising oracle (or score) from datasets. From a Bayesian perspective, they offer a realistic modeling of data priors and facilitate solving inverse problems through posterior sampling. Although many heuristic methods have been developed recently for this purpose, they lack the quantitative guarantees needed in many scientific applications. In this work, we introduce the \textbackslash textit\textbraceleft tilted transport\textbraceright{} technique, which leverages the quadratic structure of the log-likelihood in linear inverse problems in combination with the prior denoising oracle to transform the original posterior sampling problem into a new `boosted' posterior that is provably easier to sample from. We quantify the conditions under which this boosted posterior is strongly log-concave, highlighting the dependencies on the condition number of the measurement matrix and the signal-to-noise ratio. The resulting posterior sampling scheme is shown to reach the computational threshold predicted for sampling Ising models [Kunisky'23] with a direct analysis, and is further validated on high-dimensional Gaussian mixture models and scalar field \$\textbackslash varphi\textasciicircum 4\$ models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Computation,Statistics - Machine Learning}
}

@misc{calvo-ordonez2025,
  title = {Weighted {{Conditional Flow Matching}}},
  author = {{Calvo-Ordonez}, Sergio and Meunier, Matthieu and Cartea, Alvaro and Reisinger, Christoph and Gal, Yarin and {Hernandez-Lobato}, Jose Miguel},
  year = 2025,
  month = jul,
  number = {arXiv:2507.22270},
  eprint = {2507.22270},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.22270},
  urldate = {2025-10-22},
  abstract = {Conditional flow matching (CFM) has emerged as a powerful framework for training continuous normalizing flows due to its computational efficiency and effectiveness. However, standard CFM often produces paths that deviate significantly from straight-line interpolations between prior and target distributions, making generation slower and less accurate due to the need for fine discretization at inference. Recent methods enhance CFM performance by inducing shorter and straighter trajectories but typically rely on computationally expensive mini-batch optimal transport (OT). Drawing insights from entropic optimal transport (EOT), we propose Weighted Conditional Flow Matching (W-CFM), a novel approach that modifies the classical CFM loss by weighting each training pair \$(x, y)\$ with a Gibbs kernel. We show that this weighting recovers the entropic OT coupling up to some bias in the marginals, and we provide the conditions under which the marginals remain nearly unchanged. Moreover, we establish an equivalence between W-CFM and the minibatch OT method in the large-batch limit, showing how our method overcomes computational and performance bottlenecks linked to batch size. Empirically, we test our method on unconditional generation on various synthetic and real datasets, confirming that W-CFM achieves comparable or superior sample quality, fidelity, and diversity to other alternative baselines while maintaining the computational efficiency of vanilla CFM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{cardoso2023,
  title = {Monte {{Carlo}} guided {{Diffusion}} for {{Bayesian}} linear inverse problems},
  author = {Cardoso, Gabriel and Idrissi, Yazid Janati El and Corff, Sylvain Le and Moulines, Eric},
  year = 2023,
  month = oct,
  number = {arXiv:2308.07983},
  eprint = {2308.07983},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07983},
  urldate = {2025-09-18},
  abstract = {Ill-posed linear inverse problems arise frequently in various applications, from computational photography to medical imaging. A recent line of research exploits Bayesian inference with informative priors to handle the ill-posedness of such problems. Amongst such priors, score-based generative models (SGM) have recently been successfully applied to several different inverse problems. In this study, we exploit the particular structure of the prior defined by the SGM to define a sequence of intermediate linear inverse problems. As the noise level decreases, the posteriors of these inverse problems get closer to the target posterior of the original inverse problem. To sample from this sequence of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The proposed algorithm, MCGDiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems in a Bayesian setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology}
}

@misc{domingo-enrich2024a,
  title = {Stochastic {{Optimal Control Matching}}},
  author = {{Domingo-Enrich}, Carles and Han, Jiequn and Amos, Brandon and Bruna, Joan and Chen, Ricky T. Q.},
  year = 2024,
  month = oct,
  number = {arXiv:2312.02027},
  eprint = {2312.02027},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.02027},
  urldate = {2025-09-22},
  abstract = {Stochastic optimal control, which has the goal of driving the behavior of noisy systems, is broadly applicable in science, engineering and artificial intelligence. Our work introduces Stochastic Optimal Control Matching (SOCM), a novel Iterative Diffusion Optimization (IDO) technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models. That is, the control is learned via a least squares problem by trying to fit a matching vector field. The training loss, which is closely connected to the cross-entropy loss, is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field. The optimization with respect to the reparameterization matrices aims at minimizing the variance of the matching vector field. Experimentally, our algorithm achieves lower error than all the existing IDO techniques for stochastic optimal control for three out of four control problems, in some cases by an order of magnitude. The key idea underlying SOCM is the path-wise reparameterization trick, a novel technique that may be of independent interest. Code at https://github.com/facebookresearch/SOC-matching},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Mathematics - Probability,Statistics - Machine Learning}
}

@misc{domingo-enrich2024b,
  title = {Adjoint {{Matching}}: {{Fine-tuning Flow}} and {{Diffusion Generative Models}} with {{Memoryless Stochastic Optimal Control}}},
  shorttitle = {Adjoint {{Matching}}},
  author = {{Domingo-Enrich}, Carles and Drozdzal, Michal and Karrer, Brian and Chen, Ricky T. Q.},
  year = 2024,
  month = jan,
  number = {arXiv:2409.08861},
  eprint = {2409.08861},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.08861},
  urldate = {2025-09-22},
  abstract = {Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@misc{esser2024,
  title = {Scaling {{Rectified Flow Transformers}} for {{High-Resolution Image Synthesis}}},
  author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Lacey, Kyle and Goodwin, Alex and Marek, Yannik and Rombach, Robin},
  year = 2024,
  month = mar,
  number = {arXiv:2403.03206},
  eprint = {2403.03206},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03206},
  urldate = {2025-09-22},
  abstract = {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{esser2024a,
  title = {Scaling {{Rectified Flow Transformers}} for {{High-Resolution Image Synthesis}}},
  author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Lacey, Kyle and Goodwin, Alex and Marek, Yannik and Rombach, Robin},
  year = 2024,
  month = mar,
  number = {arXiv:2403.03206},
  eprint = {2403.03206},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03206},
  urldate = {2025-11-05},
  abstract = {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{ho2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = 2022,
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {2207.12598},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.12598},
  urldate = {2025-09-22},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{hu2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = 2021,
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2025-09-22},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{hyvarinena,
  title = {Estimation of {{Non-Normalized Statistical Models}} by {{Score Matching}}},
  author = {Hyvarinen, Aapo},
  year = 2005,
  journal = {Journal of Machine Learning Research},
  volume = {6},
  number = {24},
  pages = {695--709},
  abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
  langid = {english}
}

@article{janati2025,
  title = {Bridging diffusion posterior sampling and {{Monte Carlo}} methods: a survey},
  shorttitle = {Bridging diffusion posterior sampling and {{Monte Carlo}} methods},
  author = {Janati, Yazid and Moulines, Eric and Olsson, Jimmy and {Oliviero-Durmus}, Alain},
  year = 2025,
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {383},
  number = {2299},
  publisher = {The Royal Society},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2024.0331},
  urldate = {2025-09-17},
  abstract = {Diffusion models enable the synthesis of highly accurate samples from complex distributions and have become foundational in generative modelling. Recently, they have demonstrated significant potential for solving Bayesian inverse problems by serving as priors. This review offers a comprehensive overview of current methods that leverage            pre-trained            diffusion models alongside Monte Carlo methods to address Bayesian inverse problems without requiring additional training. We show that these methods primarily employ a            twisting            mechanism for the intermediate distributions within the diffusion process, guiding the simulations towards the posterior distribution. We describe how various Monte Carlo methods are then used to aid in sampling from these twisted distributions.                    This article is part of the theme issue `Generative modelling meets Bayesian inference: a new paradigm for inverse problems'.},
  copyright = {https://royalsociety.org/-/media/journals/author/Licence-to-Publish-20062019-final.pdf},
  langid = {english}
}

@misc{levine2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  year = 2018,
  month = may,
  number = {arXiv:1805.00909},
  eprint = {1805.00909},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.00909},
  urldate = {2025-09-18},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning}
}

@misc{lipman2023,
  title = {Flow {{Matching}} for {{Generative Modeling}}},
  author = {Lipman, Yaron and Chen, Ricky T. Q. and {Ben-Hamu}, Heli and Nickel, Maximilian and Le, Matt},
  year = 2023,
  month = feb,
  number = {arXiv:2210.02747},
  eprint = {2210.02747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.02747},
  urldate = {2025-09-22},
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples---which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{liu2023,
  title = {{{FLOW STRAIGHT AND FAST}}: {{LEARNING TO GENER- ATE AND TRANSFER DATA WITH RECTIFIED FLOW}}},
  author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  year = 2023,
  abstract = {We present rectified flow, a simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions {$\pi$}0 and {$\pi$}1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from {$\pi$}0 and {$\pi$}1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that, by learning a rectified flow from data, we effectively turn an arbitrary coupling of {$\pi$}0 and {$\pi$}1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, with a ``reflow'' procedure that iteratively learns a new rectified flow from the data bootstrapped from the previous one, we obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation and image-to-image translation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step. Code is available at https://github.com/gnobitab/RectifiedFlow.},
  langid = {english}
}

@misc{nichol2021a,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  year = 2021,
  month = feb,
  number = {arXiv:2102.09672},
  eprint = {2102.09672},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09672},
  urldate = {2025-10-27},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{nutz2022a,
  title = {Entropic optimal transport: convergence of potentials},
  shorttitle = {Entropic optimal transport},
  author = {Nutz, Marcel and Wiesel, Johannes},
  year = 2022,
  month = oct,
  journal = {Probability Theory and Related Fields},
  volume = {184},
  number = {1-2},
  pages = {401--424},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {0178-8051, 1432-2064},
  doi = {10.1007/s00440-021-01096-8},
  urldate = {2025-10-22},
  abstract = {We study the potential functions that determine the optimal density for {$\varepsilon$}-entropically regularized optimal transport, the so-called Schro\textasciidieresis dinger potentials, and their convergence to the counterparts in classical optimal transport, the Kantorovich potentials. In the limit {$\varepsilon$} {$\rightarrow$} 0 of vanishing regularization, strong compactness holds in L1 and cluster points are Kantorovich potentials. In particular, the Schro\textasciidieresis dinger potentials converge in L1 to the Kantorovich potentials as soon as the latter are unique. These results are proved for all continuous, integrable cost functions on Polish spaces. In the language of Schro\textasciidieresis dinger bridges, the limit corresponds to the small-noise regime.},
  copyright = {https://www.springer.com/tdm},
  langid = {english}
}

@book{oksendal2003,
  title = {Stochastic {{Differential Equations}}},
  author = {{\O}ksendal, Bernt},
  year = 2003,
  series = {Universitext},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  issn = {0172-5939, 2191-6675},
  doi = {10.1007/978-3-642-14394-6},
  urldate = {2025-09-22},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-04758-2 978-3-642-14394-6},
  langid = {english}
}

@misc{rissanen2025,
  title = {Progressive {{Tempering Sampler}} with {{Diffusion}}},
  author = {Rissanen, Severi and OuYang, RuiKang and He, Jiajun and Chen, Wenlin and Heinonen, Markus and Solin, Arno and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = 2025,
  month = oct,
  number = {arXiv:2506.05231},
  eprint = {2506.05231},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.05231},
  urldate = {2025-10-29},
  abstract = {Recent research has focused on designing neural samplers that amortize the process of sampling from unnormalized densities. However, despite significant advancements, they still fall short of the state-of-the-art MCMC approach, Parallel Tempering (PT), when it comes to the efficiency of target evaluations. On the other hand, unlike a well-trained neural sampler, PT yields only dependent samples and needs to be rerun -- at considerable computational cost -- whenever new samples are required. To address these weaknesses, we propose the Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion models sequentially across temperatures, leveraging the advantages of PT to improve the training of neural samplers. We also introduce a novel method to combine high-temperature diffusion models to generate approximate lower-temperature samples, which are minimally refined using MCMC and used to train the next diffusion model. PTSD enables efficient reuse of sample information across temperature levels while generating well-mixed, uncorrelated samples. Our method significantly improves target evaluation efficiency, outperforming diffusion-based neural samplers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{rombach2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = 2022,
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10752},
  urldate = {2025-09-22},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{sethi2019,
  title = {Optimal {{Control Theory}}: {{Applications}} to {{Management Science}} and {{Economics}}},
  shorttitle = {Optimal {{Control Theory}}},
  author = {Sethi, Suresh P.},
  year = 2019,
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98237-3},
  urldate = {2025-09-22},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-3-319-98236-6 978-3-319-98237-3},
  langid = {english}
}

@misc{shariatian2025a,
  title = {Latent {{Discrete Diffusion Models}}},
  author = {Shariatian, Dario and Durmus, Alain and Peluchetti, Stefano},
  year = 2025,
  month = oct,
  number = {arXiv:2510.18114},
  eprint = {2510.18114},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.18114},
  urldate = {2025-10-26},
  abstract = {We study discrete diffusion for language and other categorical data and focus on a common limitation of masked denoisers: reverse transitions typically factorize across positions, which can weaken joint structure and degrade quality in few-step generation. We propose \textbackslash emph\textbraceleft Latent Discrete Diffusion Models\textbraceright{} (LDDMs), which couple a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings. The latent channel provides a softer signal and carries cross-token dependencies that help resolve ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially resolve the latent and then the discrete chain conditionally on it. For both variants we derive ELBO-style objectives and discuss design choices to learn informative latents yet amenable to diffusoin modeling. In experiments, LDDMs yield improvements on unconditional generation metrics as compared to state-of-the-art masked discrete diffusion baselines, and are effective at lower sampling budgets, where unmasking many tokens per step is desirable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{sohl-dickstein2015,
  title = {Deep {{Unsupervised Learning}} using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = 2015,
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.03585},
  urldate = {2025-09-22},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning}
}

@misc{song2022,
  title = {Denoising {{Diffusion Implicit Models}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  year = 2022,
  month = oct,
  number = {arXiv:2010.02502},
  eprint = {2010.02502},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.02502},
  urldate = {2025-09-22},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 \textbackslash times\$ to \$50 \textbackslash times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{stiennon2022,
  title = {Learning to summarize from human feedback},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
  year = 2022,
  month = feb,
  number = {arXiv:2009.01325},
  eprint = {2009.01325},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.01325},
  urldate = {2025-09-22},
  abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{tancik2020,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and {Fridovich-Keil}, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
  year = 2020,
  month = jun,
  number = {arXiv:2006.10739},
  eprint = {2006.10739},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.10739},
  urldate = {2025-10-27},
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{tang2024,
  title = {Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond},
  shorttitle = {Fine-tuning of diffusion models via stochastic control},
  author = {Tang, Wenpin},
  year = 2024,
  month = mar,
  number = {arXiv:2403.06279},
  eprint = {2403.06279},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.06279},
  urldate = {2025-09-22},
  abstract = {This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized fine-tuning in the context of continuous-time diffusion models, which was recently proposed by Uehara et al. (arXiv:2402.15194, 2024). The idea is to use stochastic control for sample generation, where the entropy regularizer is introduced to mitigate reward collapse. We also show how the analysis can be extended to fine-tuning involving a general \$f\$-divergence regularizer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control}
}

@misc{uehara2024a,
  title = {Understanding {{Reinforcement Learning-Based Fine-Tuning}} of {{Diffusion Models}}: {{A Tutorial}} and {{Review}}},
  shorttitle = {Understanding {{Reinforcement Learning-Based Fine-Tuning}} of {{Diffusion Models}}},
  author = {Uehara, Masatoshi and Zhao, Yulai and Biancalani, Tommaso and Levine, Sergey},
  year = 2024,
  month = jul,
  number = {arXiv:2407.13734},
  eprint = {2407.13734},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.13734},
  urldate = {2025-09-18},
  abstract = {This tutorial provides a comprehensive survey of methods for fine-tuning diffusion models to optimize downstream reward functions. While diffusion models are widely known to provide excellent generative modeling capability, practical applications in domains such as biology require generating samples that maximize some desired metric (e.g., translation efficiency in RNA, docking score in molecules, stability in protein). In these cases, the diffusion model can be optimized not only to generate realistic samples but also to explicitly maximize the measure of interest. Such methods are based on concepts from reinforcement learning (RL). We explain the application of various RL algorithms, including PPO, differentiable optimization, reward-weighted MLE, value-weighted sampling, and path consistency learning, tailored specifically for fine-tuning diffusion models. We aim to explore fundamental aspects such as the strengths and limitations of different RL-based fine-tuning algorithms across various scenarios, the benefits of RL-based fine-tuning compared to non-RL-based approaches, and the formal objectives of RL-based fine-tuning (target distributions). Additionally, we aim to examine their connections with related topics such as classifier guidance, Gflownets, flow-based diffusion models, path integral control theory, and sampling from unnormalized distributions such as MCMC. The code of this tutorial is available at https://github.com/masa-ue/RLfinetuning\_Diffusion\_Bioseq},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning}
}

@misc{uehara2024b,
  title = {Fine-{{Tuning}} of {{Continuous-Time Diffusion Models}} as {{Entropy-Regularized Control}}},
  author = {Uehara, Masatoshi and Zhao, Yulai and Black, Kevin and Hajiramezanali, Ehsan and Scalia, Gabriele and Diamant, Nathaniel Lee and Tseng, Alex M. and Biancalani, Tommaso and Levine, Sergey},
  year = 2024,
  month = feb,
  number = {arXiv:2402.15194},
  eprint = {2402.15194},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.15194},
  urldate = {2025-09-22},
  abstract = {Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose a substantial obstacle. To address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural SDEs. We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = 2023,
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-09-22},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{wallace2023,
  title = {Diffusion {{Model Alignment Using Direct Preference Optimization}}},
  author = {Wallace, Bram and Dang, Meihua and Rafailov, Rafael and Zhou, Linqi and Lou, Aaron and Purushwalkam, Senthil and Ermon, Stefano and Xiong, Caiming and Joty, Shafiq and Naik, Nikhil},
  year = 2023,
  month = nov,
  number = {arXiv:2311.12908},
  eprint = {2311.12908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.12908},
  urldate = {2025-09-22},
  abstract = {Large language models (LLMs) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback (RLHF) methods to make them better aligned with users' preferences. In contrast to LLMs, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-DPO, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-DPO is adapted from the recently developed Direct Preference Optimization (DPO), a simpler alternative to RLHF which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate DPO to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our fine-tuned base model significantly outperforms both base SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses AI feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}

@misc{xie2025,
  title = {Slow {{Transition}} to {{Low-Dimensional Chaos}} in {{Heavy-Tailed Recurrent Neural Networks}}},
  author = {Xie, Yi and Mihalas, Stefan and Ku{\'s}mierz, {\L}ukasz},
  year = 2025,
  month = may,
  number = {arXiv:2505.09816},
  eprint = {2505.09816},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.09816},
  urldate = {2025-09-23},
  abstract = {Growing evidence suggests that synaptic weights in the brain follow heavy-tailed distributions, yet most theoretical analyses of recurrent neural networks (RNNs) assume Gaussian connectivity. We systematically study the activity of RNNs with random weights drawn from biologically plausible L\textbackslash 'evy alpha-stable distributions. While mean-field theory for the infinite system predicts that the quiescent state is always unstable -- implying ubiquitous chaos -- our finite-size analysis reveals a sharp transition between quiescent and chaotic dynamics. We theoretically predict the gain at which the system transitions from quiescent to chaotic dynamics, and validate it through simulations. Compared to Gaussian networks, heavy-tailed RNNs exhibit a broader parameter regime near the edge of chaos, namely a slow transition to chaos. However, this robustness comes with a tradeoff: heavier tails reduce the Lyapunov dimension of the attractor, indicating lower effective dimensionality. Our results reveal a biologically aligned tradeoff between the robustness of dynamics near the edge of chaos and the richness of high-dimensional neural activity. By analytically characterizing the transition point in finite-size networks -- where mean-field theory breaks down -- we provide a tractable framework for understanding dynamics in realistically sized, heavy-tailed neural circuits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Physics - Biological Physics,Quantitative Biology - Neurons and Cognition}
}

@misc{xu2023b,
  title = {{{ImageReward}}: {{Learning}} and {{Evaluating Human Preferences}} for {{Text-to-Image Generation}}},
  shorttitle = {{{ImageReward}}},
  author = {Xu, Jiazheng and Liu, Xiao and Wu, Yuchen and Tong, Yuxuan and Li, Qinkai and Ding, Ming and Tang, Jie and Dong, Yuxiao},
  year = 2023,
  month = dec,
  number = {arXiv:2304.05977},
  eprint = {2304.05977},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.05977},
  urldate = {2025-04-28},
  abstract = {We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. To begin with, we build ImageReward -- the first general-purpose text-to-image human preference reward model -- to effectively encode human preferences. Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis. On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer. Both automatic and human evaluation support ReFL's advantages over compared methods. All code and datasets are provided at \textbackslash url\textbraceleft https://github.com/THUDM/ImageReward\textbraceright.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{yang2024a,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  year = 2024,
  month = dec,
  number = {arXiv:2209.00796},
  eprint = {2209.00796},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.00796},
  urldate = {2025-09-22},
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{zheng2023,
  title = {Guided {{Flows}} for {{Generative Modeling}} and {{Decision Making}}},
  author = {Zheng, Qinqing and Le, Matt and Shaul, Neta and Lipman, Yaron and Grover, Aditya and Chen, Ricky T. Q.},
  year = 2023,
  month = dec,
  number = {arXiv:2311.13443},
  eprint = {2311.13443},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.13443},
  urldate = {2025-09-22},
  abstract = {Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \textbackslash emph\textbraceleft Guided Flows\textbraceright{} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning}
}

@misc{ziegler2020,
  title = {Fine-{{Tuning Language Models}} from {{Human Preferences}}},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  year = 2020,
  month = jan,
  number = {arXiv:1909.08593},
  eprint = {1909.08593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.08593},
  urldate = {2025-09-22},
  abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{dhariwal2021,
  title = {Diffusion Models Beat {{GANs}} on Image Synthesis},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  year = 2021,
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {8780--8794},
  publisher = {Curran Associates, Inc.},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman}
}

@inproceedings{song2019,
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  author = {Song, Yang and Ermon, Stefano},
  year = 2019,
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'Alch\'{e}-Buc, F. and Fox, E. and Garnett, R.}
}

@inproceedings{song2021ddim,
  title = {Denoising Diffusion Implicit Models},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  year = 2021,
  booktitle = {International Conference on Learning Representations}
}

@inproceedings{anonymous2025tilt,
  title = {Tilt Matching for Scalable Sampling and Fine-Tuning},
  author = {Anonymous},
  year = 2025,
  booktitle = {Submitted to The Fourteenth International Conference on Learning Representations},
  url = {https://openreview.net/forum?id=tT7CXL3I9C},
  note = {under review}
}

@article{wiese2020quant,
  title={Quant GANs: Deep generation of financial time series},
  author={Wiese, Magnus and Knobloch, Robert and Korn, Ralf and Kretschmer, Peter},
  journal={Quantitative Finance},
  volume={20},
  number={9},
  pages={1419--1440},
  year={2020}
}

@article{lafon2023vae,
  title={On the use of variational autoencoders for extreme values modelling},
  author={Lafon, Marco and Naveau, Philippe and Leblois, Anthony},
  journal={arXiv preprint arXiv:2303.11536},
  year={2023}
}

@article{zhang2024flexibleefficientspatialextremes,
  title={Flexible and efficient spatial extremes emulation via variational autoencoders},
  author={Zhang, Likun and Bray, Andrew and Guinness, Joseph},
  journal={arXiv preprint arXiv:2407.02868},
  year={2024}
}

@article{FOUGERES2013109,
  title={Multivariate extremes and max-linear models},
  author={Fougères, Anne-Laure and Mercadier, Cécile},
  journal={ESAIM: Probability and Statistics},
  volume={17},
  pages={109--123},
  year={2013}
}

@article{janssen2020k,
  title={K-means clustering of extremes},
  author={Janßen, Anja and Wan, Phyllis},
  journal={Electronic Journal of Statistics},
  volume={14},
  number={1},
  pages={1211--1233},
  year={2020}
}

@article{HuslerReiss1989,
  title={Maxima of normal random vectors: between independence and complete dependence},
  author={Hüsler, Jürg and Reiss, Rolf-Dieter},
  journal={Statistics \& Probability Letters},
  volume={7},
  number={4},
  pages={283--286},
  year={1989}
}

@article{mainik2015portfolio,
  title={Portfolio optimization for heavy-tailed assets: Extreme Risk Index vs. Markowitz},
  author={Mainik, Georg and Mitov, Georgi and Rüschendorf, Ludger},
  journal={Journal of Empirical Finance},
  volume={32},
  pages={115--134},
  year={2015}
}
